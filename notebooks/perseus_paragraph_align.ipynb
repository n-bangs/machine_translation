{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(soup):\n",
    "    l = []\n",
    "    for n in soup:\n",
    "        try:\n",
    "            l.append(int(n['n']))\n",
    "        except:\n",
    "            l.append(-1)  \n",
    "    return remove_duplicates(l)\n",
    "\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    i = 0\n",
    "    while i+1 < len(l):\n",
    "        if l[i] == l[i+1]:\n",
    "            l[i+1] += 1\n",
    "        if l[i] > l[i+1] and l[i+1] != -1:\n",
    "            l[i] = l[i+1]-1\n",
    "        i += 1\n",
    "    return l\n",
    "\n",
    "def match(large,small):\n",
    "    l = make_list(large)\n",
    "    s = make_list(small)\n",
    "    li = 0\n",
    "    si = 0\n",
    "    groups = []\n",
    "    #print(l,s)\n",
    "    while si < len(s) and li < len(l):\n",
    "        if s[si] == -1:\n",
    "            si += 1\n",
    "            continue\n",
    "        if l[li] == -1:\n",
    "            li += 1\n",
    "            continue\n",
    "        lgroup = []\n",
    "        sgroup = []\n",
    "        if l[li] == s[si]:\n",
    "            lgroup.append(li)\n",
    "            sgroup.append(si)\n",
    "              \n",
    "        if si+1 < len(s) and li+1 < len(l): \n",
    "            if s[si+1] == -1:\n",
    "                si += 1\n",
    "            \n",
    "            if l[li+1] == -1:\n",
    "                li += 1\n",
    "            \n",
    "            \n",
    "            if l[li+1] != s[si+1]:\n",
    "                while l[li+1] != s[si+1]:\n",
    "                    if si+1 < len(s) and li+1 < len(l):\n",
    "                        if l[li+1] > s[si+1]:\n",
    "                            sgroup.append(si+1)\n",
    "                            si += 1\n",
    "                            if s[si] == -1:\n",
    "                                if si+1 < len(s):\n",
    "                                    si += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                        else:\n",
    "                            lgroup.append(li+1)\n",
    "                            li += 1\n",
    "                            if l[li] == -1:\n",
    "                                if li+1 < len(l):\n",
    "                                    li += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                    else:\n",
    "                        break\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup])\n",
    "            else:\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup]) \n",
    "        else:\n",
    "            break        \n",
    "    \n",
    "    groups.append([sgroup,lgroup])\n",
    "    return groups\n",
    "        \n",
    "\n",
    "def align_soups(m,small,large):\n",
    "    s_alignments = []\n",
    "    l_alignments = []\n",
    "    for g in m:\n",
    "        s = []\n",
    "        l = []\n",
    "        for i in g[0]:\n",
    "            s.append(small[i])\n",
    "        for i in g[1]:\n",
    "            l.append(large[i])\n",
    "        s_alignments.append(s)\n",
    "        l_alignments.append(l)   \n",
    "    return s_alignments,l_alignments\n",
    "\n",
    "\n",
    "def get_pairs(latin_soup, eng_soup, tag, typename, typos):\n",
    "    \"\"\" Given two soups, returns soups of equal size, with each element corresponding to the same\n",
    "        element in the other soup.\n",
    "        Lists may contain further lists of soups which are aligned with one another, they are combined into one list\n",
    "        in the first step of this function.\n",
    "    \"\"\"\n",
    "    lat_sect,eng_sect = [], []\n",
    "    \n",
    "    for i in range(len(latin_soup)):\n",
    "        lat_sect += latin_soup[i].findAll(tag, {typename: typos})\n",
    "    for i in range(len(eng_soup)):\n",
    "        eng_sect += eng_soup[i].findAll(tag, {typename: typos})\n",
    "\n",
    "    if len(lat_sect) > len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].parent['n'])\n",
    "            m = match(lat_sect,eng_sect)\n",
    "            eng_alignments,lat_alignments = align_soups(m,eng_sect,lat_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    elif len(lat_sect) < len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "            m = match(eng_sect,lat_sect)\n",
    "            lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    else:\n",
    "        if eng_sect[0]['n'] == 'intro':\n",
    "            try:\n",
    "                #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "                m = match(eng_sect,lat_sect)\n",
    "                lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "                return lat_alignments,eng_alignments\n",
    "            except:\n",
    "                print(\"error with {}, {}\".format(tag,typos))\n",
    "        else:\n",
    "            \n",
    "            return lat_sect, eng_sect\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def align_pars(lsentences,esentences):\n",
    "    \n",
    "    if lsentences[-1] == '':\n",
    "        lsentences = lsentences[:-1]\n",
    "    if esentences[-1] == '':\n",
    "        esentences = esentences[:-1]\n",
    "        \n",
    "    if len(lsentences) > len(esentences):\n",
    "        alignments = align(lsentences,esentences)\n",
    "        target = esentences\n",
    "        source = lsentences\n",
    "    elif len(lsentences) < len(esentences):\n",
    "        alignments = align(esentences,lsentences)\n",
    "        target = lsentences\n",
    "        source = esentences\n",
    "    else:\n",
    "        return zip(lsentences,esentences)\n",
    "    \n",
    "    sentence_tuples = []\n",
    "    for k in alignments.keys():\n",
    "        sentence_tuples.append((target[k], [source[j] for j in alignments[k]]))\n",
    "        \n",
    "    return sentence_tuples\n",
    "          \n",
    "\n",
    "def get_text(soup):\n",
    "    \"\"\"Expects a soup in the form of a find('p') selection\"\"\"\n",
    "    s = []\n",
    "    for c in soup.children:\n",
    "        if c.name in [None,'corr', 'name']:\n",
    "            try:\n",
    "                s.append(c.text.strip())\n",
    "            except:\n",
    "                s.append(c.strip())\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_lemma(word):\n",
    "    try:\n",
    "        word = re.sub('[0-9]+', '', word)\n",
    "        word = re.sub('[-_]+', '', word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_lemmata_defs(latin):\n",
    "    words = lemmatizer.lemmatize(latin)\n",
    "    ldefs = []\n",
    "    unmatched = []\n",
    "\n",
    "    for word in words:\n",
    "        word = clean_lemma(word)\n",
    "        try:\n",
    "            ldefs.append(defs[word]['definition'].split(','))\n",
    "        except:\n",
    "            unmatched.append(word)\n",
    "    return ldefs,unmatched\n",
    "\n",
    "\n",
    "def get_defs(ldefs):\n",
    "    lldefs=[]\n",
    "    for l in ldefs:\n",
    "        lldefs += t(l[0])\n",
    "    return lldefs\n",
    "\n",
    "\n",
    "def get_match_count(english,latin):\n",
    "    \n",
    "    ldefs,unmatched = get_lemmata_defs(latin)\n",
    "    ldefs = get_defs(ldefs)\n",
    "    count = 0\n",
    "    for word in t(english):\n",
    "        if word in string.punctuation:\n",
    "            continue\n",
    "        if word in ldefs and word not in stop:\n",
    "            count += 1\n",
    "        if word in unmatched:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def align(source,target):\n",
    "    \"\"\" Takes a paragraph of sentences, source, and maps each sentence to a sentence in the \n",
    "        target paragraph. \n",
    "        \n",
    "        Returns a dictionary with the target sentence indices as keys, and the sentences in the source paragraph\n",
    "        which correspond to them as values.\n",
    "    \"\"\"\n",
    "    tsize = len(target)\n",
    "    ssize = len(source)\n",
    "    alignments = defaultdict(list)\n",
    "    alignments[0] = [0]\n",
    "    i = 1\n",
    "    j = 1\n",
    "    while tsize < ssize and i < len(source) and j < tsize:\n",
    "        max_count = get_match_count(source[i],target[j])\n",
    "        max_i = j\n",
    "        if j >= tsize-1:\n",
    "            rge = [j-1]\n",
    "        else:\n",
    "            rge = [j-1,j+1]\n",
    "        for r in rge: \n",
    "            c = get_match_count(source[i],target[r])\n",
    "            if c > max_count:\n",
    "                max_count = c\n",
    "                max_i = r\n",
    "\n",
    "        if max_i != j:\n",
    "            ssize -= 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        alignments[max_i].append(i)\n",
    "        i += 1\n",
    "        \n",
    "  \n",
    "    while j < tsize:\n",
    "        alignments[j].append(i)\n",
    "        j += 1\n",
    "        i += 1\n",
    "    alignments[j-1].extend([k for k in range(i,len(source))])\n",
    "    \n",
    "    return alignments\n",
    "\n",
    "def get_sec_alignments(lpar,epar):\n",
    "    if len(lpar) > len(epar):\n",
    "        if len(epar) > 1:\n",
    "            alignments = align(lpar,epar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(lpar[v])\n",
    "                aligned.append((group,epar[k]))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [('. '.join(lpar),epar[0])] \n",
    "            except:\n",
    "                print('error, {}'.format(epar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    elif len(epar) > len(lpar):\n",
    "        if len(lpar) > 1:\n",
    "            alignments = align(epar,lpar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(epar[v])\n",
    "                aligned.append((lpar[k],group))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [(lpar[0],'. '.join(epar))]\n",
    "            except:\n",
    "                print('error, {}'.format(lpar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    else:\n",
    "        aligned = list(zip(lpar,epar))\n",
    "        \n",
    "    return aligned\n",
    "\n",
    "def get_sentences(chap_soup, sect_soup):\n",
    "    t = bs4.element.Tag\n",
    "    sect = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    c = 0\n",
    "    children = list(chap_soup.children)\n",
    "    soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "    c += 1\n",
    "    while type(soup) != t:\n",
    "        soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "        c += 1\n",
    "    i += 1\n",
    "    while c < len(children):\n",
    "        while soup.next_sibling != None:\n",
    "            soup = soup.next_sibling\n",
    "            if soup.name == 'milestone':\n",
    "                try:\n",
    "                    if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                        i += 1\n",
    "                        sentences.append(' '.join(sect))\n",
    "                        sect = []\n",
    "                except:\n",
    "                    soup = soup.next_sibling\n",
    "                    i += 1\n",
    "            if soup.name in ['corr', 'name','hi','quote','reg', 'l', 'lg', 'placeName']:\n",
    "                    sect.append(soup.text.strip())\n",
    "            elif soup.name == None:\n",
    "                    sect.append(soup.strip())\n",
    "                    \n",
    "        sentences.append(' '.join(sect))\n",
    "        sect = []\n",
    "        try:\n",
    "            soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "            c += 1\n",
    "            while type(soup) != t and c < len(children):\n",
    "                soup = children[c].find('milestone')\n",
    "                c += 1\n",
    "            i += 1\n",
    "        except:\n",
    "            continue\n",
    "    sentences.append(' '.join(sect))\n",
    "    sect = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aulus Gellius ###\n",
    "\tgel_eng.xml , gel_lat.xml  \n",
    "### Quintilian  \n",
    "\tquint.butler1-3_eng.xml , quint.butler1-3_lat.xml  \n",
    "\tquint.butler10-12_eng.xml , quint.butler10-12_lat.xml  \n",
    "\tquint.butler4-6_eng.xml , quint.butler4-6_lat.xml  \n",
    "\tquint.butler7-9_eng.xml , quint.butler7-9_lat.xml  \n",
    "### Celsus  \n",
    "\tcels_eng.xml , cels_lat.xml  \n",
    "### Ammianus Marcellinus  \n",
    "\tamm_eng.xml , amm_lat.xml  \n",
    "### M. Annaeus Lucanus  \n",
    "\tpharsalia_eng.xml , pharsalia_lat.xml  \n",
    "### Q. Horatius Flaccus (Horace)  \n",
    "\thor.ap_eng.xml , hor.ap_lat.xml  \n",
    "\thor.sat_eng.xml , hor.sat_lat.xml  \n",
    "### M. Tullius Cicero  \n",
    "\tcic.amic_falc_eng.xml , cic.amic_falc_lat.xml  \n",
    "\tcic.div_falc_eng.xml , cic.div_falc_lat.xml  \n",
    "\tcic.off_eng.xml , cic.off_lat.xml  \n",
    "\tcic.sen_falc_eng.xml , cic.sen_falc_lat.xml  \n",
    "### Seneca  \n",
    "\tsen.apoc_eng.xml , sen.apoc_lat.xml  \n",
    "### P. Vergilius Maro  \n",
    "\tverg.ecl_eng.xml , verg.ecl_lat.xml  \n",
    "\tverg.g_eng.xml , verg.g_lat.xml  \n",
    "### Vitruvius Pollio  \n",
    "\tvitruv_eng.xml , vitruv_lat.xml  \n",
    "### Sallust  \n",
    "\tsallust.catil_eng.xml , sallust.catil_lat.xml  \n",
    "\tsallust.jugur_eng.xml , sallust.jugur_lat.xml  \n",
    "### T. Maccius Plautus  \n",
    "\tpl.am_eng.xml , pl.am_lat.xml  \n",
    "\tpl.as_eng.xml , pl.as_lat.xml  \n",
    "\tpl.aul_eng.xml , pl.aul_lat.xml  \n",
    "\tpl.bac_eng.xml , pl.bac_lat.xml  \n",
    "\tpl.capt_eng.xml , pl.capt_lat.xml  \n",
    "\tpl.cas_eng.xml , pl.cas_lat.xml  \n",
    "\tpl.cist_eng.xml , pl.cist_lat.xml  \n",
    "\tpl.cur_eng.xml , pl.cur_lat.xml  \n",
    "\tpl.epid_eng.xml , pl.epid_lat.xml  \n",
    "\tpl.men_eng.xml , pl.men_lat.xml  \n",
    "\tpl.mer_eng.xml , pl.mer_lat.xml  \n",
    "\tpl.mil_eng.xml , pl.mil_lat.xml  \n",
    "\tpl.mos_eng.xml , pl.mos_lat.xml  \n",
    "\tpl.per_eng.xml , pl.per_lat.xml  \n",
    "\tpl.poen_eng.xml , pl.poen_lat.xml  \n",
    "\tpl.ps_eng.xml , pl.ps_lat.xml  \n",
    "\tpl.rud_eng.xml , pl.rud_lat.xml  \n",
    "\tpl.st_eng.xml , pl.st_lat.xml  \n",
    "\tpl.trin_eng.xml , pl.trin_lat.xml  \n",
    "\tpl.truc_eng.xml , pl.truc_lat.xml  \n",
    "### Lucretius  \n",
    "\tlucretius_eng.xml , lucretius_lat.xml  \n",
    "### P. Terentius Afer (Terence)  \n",
    "\tad_eng.xml , ad_lat.xml  \n",
    "\tan_eng.xml , an_lat.xml  \n",
    "\teu_eng.xml , eu_lat.xml  \n",
    "\thau_eng.xml , hau_lat.xml  \n",
    "\thec_eng.xml , hec_lat.xml  \n",
    "\tph_eng.xml , ph_lat.xml  \n",
    "### Petronius  \n",
    "\tpetr_eng.xml , petr_lat.xml  \n",
    "### C. Julius Caesar  \n",
    "\tcaes.bc_eng.xml , caes.bc_lat.xml  \n",
    "\tcaes.bg_eng.xml , caes.bg_lat.xml  \n",
    "### P. Ovidius Naso  \n",
    "\tovid.am_eng.xml , ovid.am_lat.xml  \n",
    "\tovid.met_eng.xml , ovid.met_lat.xml  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = wordpunct_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "f = open('lemmas.json')\n",
    "defs = json.loads(f.read())\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "tokenizer = WordTokenizer('latin')\n",
    "s_tokenizer = TokenizeSentence('latin')\n",
    "\n",
    "f = open('../Latin/Ammianus/opensource/amm_lat.xml')\n",
    "latin_soup = BeautifulSoup(f.read(),'lxml')\n",
    "f.close()\n",
    "\n",
    "f = open('../Latin/Ammianus/opensource/amm_eng.xml')\n",
    "eng_soup = BeautifulSoup(f.read(),'lxml')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "units = []\n",
    "for s in latin_soup.find('encodingdesc').findAll('state'):\n",
    "    units.append(s['unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'chapter', 'section', 'page']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat,eng = get_pairs([latin_soup],[eng_soup],'div1','type','book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lat,eng = get_pairs([latin_soup],[eng_soup],'div1','type','chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lchaps = []\n",
    "echaps = []\n",
    "for i in range(len(lat)):\n",
    "    try:\n",
    "        p = get_pairs([lat[i]],[eng[i]],'div2','type','chapter')\n",
    "        if type(p[0][0]) == list:\n",
    "            lchaps.append([s[0] for s in p[0]])\n",
    "            echaps.append([s[0] for s in p[1]])\n",
    "        else:\n",
    "            lchaps.append(p[0])\n",
    "            echaps.append(p[1])\n",
    "    except:\n",
    "        print('error with {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsoup = []\n",
    "esoup = []\n",
    "for b in range(len(lchaps)):\n",
    "    lchap_soup = []\n",
    "    i = 0\n",
    "    soup = re.split(r'{}'.format(lchaps[b][i]),str(lat[b]))[1]\n",
    "    i += 1\n",
    "\n",
    "    while i < len(lchaps[b]):\n",
    "        soups = re.split(r'{}'.format(lchaps[b][i]),soup)\n",
    "        lchap_soup.append(BeautifulSoup(soups[0],'lxml'))\n",
    "        soup = soups[1]\n",
    "        i += 1\n",
    "\n",
    "    lchap_soup.append(BeautifulSoup(soup,'lxml'))\n",
    "\n",
    "    echap_soup = []\n",
    "    i = 0\n",
    "    soup = re.split(r'{}'.format(echaps[b][i]),str(eng[b]))[1]\n",
    "    i += 1\n",
    "\n",
    "    while i < len(lchaps[b]):\n",
    "        soups = re.split(r'{}'.format(echaps[b][i]),soup)\n",
    "        echap_soup.append(BeautifulSoup(soups[0],'lxml'))\n",
    "        soup = soups[1]\n",
    "        i += 1\n",
    "\n",
    "    echap_soup.append(BeautifulSoup(soup,'lxml'))\n",
    "    \n",
    "    lsoup.append(lchap_soup)\n",
    "    esoup.append(echap_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-269496c0db0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlsects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mesects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlsects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mesects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lsoup' is not defined"
     ]
    }
   ],
   "source": [
    "lsects = []\n",
    "esects = []\n",
    "for i in range(len(lsoup)):\n",
    "    lsects.append([])\n",
    "    esects.append([])\n",
    "    for c in range(len(lsoup[i])):\n",
    "        try:\n",
    "            p = get_pairs([lsoup[i][c]],[lsoup[i][c]],'milestone','unit','section')\n",
    "            lsects[i].append(p[0])\n",
    "            esects[i].append(p[1])\n",
    "        except:\n",
    "            print('error with {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n",
      "error with milestone, section\n",
      "error with 1\n"
     ]
    }
   ],
   "source": [
    "lsects = []\n",
    "esects = []\n",
    "for i in range(len(lat)):\n",
    "    lsects.append([])\n",
    "    esects.append([])\n",
    "    for c in range(len(lchaps[i])):\n",
    "        try:\n",
    "            p = get_pairs([lat[i]],[eng[i]],'milestone','unit','section')\n",
    "            lsects[i].append(p[0])\n",
    "            esects[i].append(p[1])\n",
    "        except:\n",
    "            print('error with {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sentences_2(chap_soup,sect_soup):\n",
    "    typ = bs4.element.Tag\n",
    "    sect = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    children = list(chap_soup.children)\n",
    "    soup = children[0].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "    \n",
    "    while True:\n",
    "        while soup.next_sibling != None:\n",
    "            soup = soup.next_sibling\n",
    "            if soup.name == 'milestone':\n",
    "                try:\n",
    "                    if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                        i += 1\n",
    "                        sentences.append(' '.join(sect))\n",
    "                        sect = []\n",
    "                except:\n",
    "                    soup = soup.next_sibling\n",
    "                    #i += 1\n",
    "            if soup.name in ['corr', 'name','hi','quote','reg', 'l']:\n",
    "                    sect.append(soup.text.strip())\n",
    "            elif soup.name == None:\n",
    "                    sect.append(soup.strip())\n",
    "            if type(soup) == typ:       \n",
    "                if list(soup.children) != None and list(soup.children) != []:\n",
    "                    try:\n",
    "                        if 'milestone' in [c.name for c in list(soup.children)]:\n",
    "                            soup = soup.find('milestone')\n",
    "                            try:\n",
    "                                if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                                    i += 1\n",
    "                                    sentences.append(' '.join(sect))\n",
    "                                    sect = []\n",
    "                            except:\n",
    "                                soup = soup.next_sibling\n",
    "                    except:\n",
    "                        sect += soup\n",
    "        soup = soup.find_next()\n",
    "        if soup == None:\n",
    "            break\n",
    "            \n",
    "        if soup.name == 'milestone':\n",
    "            try:\n",
    "                if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                    i += 1\n",
    "                    sentences.append(' '.join(sect))\n",
    "                    sect = []\n",
    "            except:\n",
    "                soup = soup.next_sibling\n",
    "                #i += 1\n",
    "        if soup.name in ['corr', 'name','hi','quote','reg', 'l']:\n",
    "            sect.append(soup.text.strip())\n",
    "        elif soup.name == None:\n",
    "            sect.append(soup.strip())\n",
    "            \n",
    "    sentences.append(' '.join(sect))\n",
    "    sect = []\n",
    "\n",
    "    #sentences.append(' '.join(sect))\n",
    "    #sect = []\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lchap_1 = []\n",
    "echap_1 = []\n",
    "#for b in range(len(lchaps)):\n",
    "#lchap_soup = []\n",
    "i = 0\n",
    "soup = re.split(r'{}'.format(lsects[0][0][i]),str(lat[0]),maxsplit=1)[1]\n",
    "i += 1\n",
    "\n",
    "while i < len(lsects[0][0]):\n",
    "    soups = re.split(r'{}'.format(lsects[0][0][i]),soup,maxsplit=1)\n",
    "    lchap_1.append(BeautifulSoup(soups[0],'lxml'))\n",
    "    try:\n",
    "        soup = soups[1]\n",
    "    except:\n",
    "        soup = soups[0]\n",
    "    i += 1\n",
    "\n",
    "lchap_1.append(BeautifulSoup(soup,'lxml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#echap_soup = []\n",
    "i = 0\n",
    "soup = re.split(r'{}'.format(esects[0][0][i]),str(eng[0]),maxsplit=1)[1]\n",
    "i += 1\n",
    "\n",
    "while i < len(esects[0][0]):\n",
    "    soups = re.split(r'{}'.format(esects[0][0][i]),soup,maxsplit=1)\n",
    "    echap_1.append(BeautifulSoup(soups[0],'lxml'))\n",
    "    try:\n",
    "        soup = soups[1]\n",
    "    except:\n",
    "        soup = soups[0]\n",
    "    i += 1\n",
    "\n",
    "echap_1.append(BeautifulSoup(soup,'lxml'))\n",
    "\n",
    "#lsoup.append(lchap_soup)\n",
    "#esoup.append(echap_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f079784c08b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msec_alignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lsoup' is not defined"
     ]
    }
   ],
   "source": [
    "alignments = []\n",
    "for b in range(len(lsoup)):\n",
    "    print('b: ',b)\n",
    "    sec_alignments = []\n",
    "    for c in range(len(lsects[b])):\n",
    "        lch = lsoup[b][c]\n",
    "        lsec = lsects[b][c][0]\n",
    "        ech = esoup[b][c]\n",
    "        esec = esects[b][c][0]\n",
    "        try:\n",
    "            ls = get_sentences_2(lch,lsec)\n",
    "            es = get_sentences_2(ech,esec)\n",
    "        except:\n",
    "            print('error, {},{}'.format(b,c))\n",
    "        ls = [re.sub(r'[0-9]{1,2}\\.','',l) for l in ls]\n",
    "        es = [re.sub(r'[0-9]{1,2}\\.','',e) for e in es]\n",
    "        ls = [re.sub(r'\\. \\. \\.','',l) for l in ls]\n",
    "        es = [re.sub(r'\\. \\. \\.','',e) for e in es]\n",
    "        ls =  [re.sub('[:;,]','',l) for l in ls]\n",
    "        es = [ re.sub('[:;,]','',e) for e in es]\n",
    "\n",
    "        epar = [sent_tokenize(e.replace('\\n',' ')) for e in es]\n",
    "        lpar = [s_tokenizer.tokenize_sentences(l.replace('\\n',' ')) for l in ls]\n",
    "        if len(lpar) != len(epar):\n",
    "            print(len(lpar),len(epar))\n",
    "            continue\n",
    "        \n",
    "        for p in range(len(lpar)):\n",
    "            sec_alignments.append(get_sec_alignments(lpar[p],epar[p]))\n",
    "    alignments.append(sec_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsects = []\n",
    "esects = []\n",
    "for i in range(len(lchaps)):\n",
    "    lsects.append([])\n",
    "    esects.append([])\n",
    "    for c in range(len(lchaps[i])):\n",
    "        try:\n",
    "            p = get_pairs([lchaps[i][c]],[echaps[i][c]],'milestone','unit','section')\n",
    "            lsects[i].append(p[0])\n",
    "            esects[i].append(p[1])\n",
    "        except:\n",
    "            print('error with {},{}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  0  --  11 11\n",
      "b:  1  --  13 13\n",
      "b:  2  --  12 12\n",
      "b:  3  --  14 14\n",
      "b:  4  --  10 10\n",
      "b:  5  --  13 13\n",
      "b:  6  --  11 11\n",
      "b:  7  --  16 16\n",
      "b:  8  --  16 16\n",
      "b:  9  --  6 6\n",
      "b:  10  --  8 8\n",
      "b:  11  --  10 10\n",
      "b:  12  --  10 10\n",
      "b:  13  --  12 12\n",
      "b:  14  --  6 6\n",
      "b:  15  --  6 6\n",
      "b:  16  --  10 10\n",
      "b:  17  --  16 16\n",
      "b:  18  --  6 6\n",
      "b:  19  --  10 10\n"
     ]
    }
   ],
   "source": [
    "for b in range(len(lchaps)):\n",
    "    print('b: ',b, ' -- ', len(lchaps[b]),len(echaps[b]))\n",
    "    #for c in range(len(lchaps[b])):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  0\n",
      "c:  0\n",
      "10 11\n",
      "c:  1\n",
      "8 11\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "b:  1\n",
      "b:  2\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "c:  11\n",
      "b:  3\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "error, []\n",
      "error, []\n",
      "error, []\n",
      "error, []\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "c:  11\n",
      "c:  12\n",
      "c:  13\n",
      "b:  4\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "b:  5\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "3 2\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "c:  11\n",
      "c:  12\n",
      "b:  6\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "b:  7\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "1 9\n",
      "c:  9\n",
      "c:  10\n",
      "c:  11\n",
      "c:  12\n",
      "c:  13\n",
      "c:  14\n",
      "c:  15\n",
      "b:  8\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "3 2\n",
      "c:  10\n",
      "c:  11\n",
      "c:  12\n",
      "c:  13\n",
      "c:  14\n",
      "c:  15\n",
      "b:  9\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "b:  10\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "15 14\n",
      "c:  4\n",
      "c:  5\n",
      "9 6\n",
      "c:  6\n",
      "c:  7\n",
      "b:  11\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "16 19\n",
      "c:  8\n",
      "c:  9\n",
      "b:  12\n",
      "c:  0\n",
      "c:  1\n",
      "3 4\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "b:  13\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "c:  11\n",
      "b:  14\n",
      "c:  0\n",
      "21 19\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "12 13\n",
      "b:  15\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "4 3\n",
      "c:  3\n",
      "3 2\n",
      "c:  4\n",
      "c:  5\n",
      "7 6\n",
      "b:  16\n",
      "c:  0\n",
      "c:  1\n",
      "13 9\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "7 1\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "b:  17\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "c:  9\n",
      "c:  10\n",
      "c:  11\n",
      "c:  12\n",
      "c:  13\n",
      "c:  14\n",
      "c:  15\n",
      "b:  18\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "b:  19\n",
      "c:  0\n",
      "c:  1\n",
      "c:  2\n",
      "c:  3\n",
      "c:  4\n",
      "c:  5\n",
      "9 8\n",
      "c:  6\n",
      "c:  7\n",
      "c:  8\n",
      "3 2\n",
      "c:  9\n"
     ]
    }
   ],
   "source": [
    "alignments = []\n",
    "for b in range(len(lchaps)):\n",
    "    print('b: ',b)\n",
    "    for c in range(len(lsects[b])):\n",
    "        print('c: ',c)\n",
    "        lch = lchaps[b][c]\n",
    "        lsec = lsects[b][c]\n",
    "        ech = echaps[b][c]\n",
    "        esec = esects[b][c]\n",
    "        \n",
    "        try:\n",
    "            ls = get_sentences(lch,lsec)\n",
    "            es = get_sentences(ech,esec)\n",
    "        except:\n",
    "            pass\n",
    "        ls = [re.sub(r'[0-9]{1,2}\\.','',l) for l in ls]\n",
    "        es = [re.sub(r'[0-9]{1,2}\\.','',e) for e in es]\n",
    "        ls = [re.sub(r'\\. \\. \\.','',l) for l in ls]\n",
    "        es = [re.sub(r'\\. \\. \\.','',e) for e in es]\n",
    "        ls =  [re.sub('[:;,]','',l) for l in ls]\n",
    "        es = [ re.sub('[:;,]','',e) for e in es]\n",
    "\n",
    "        epar = [sent_tokenize(e.replace('\\n',' ')) for e in es]\n",
    "        lpar = [s_tokenizer.tokenize_sentences(l.replace('\\n',' ')) for l in ls]\n",
    "        if len(lpar) != len(epar):\n",
    "            print(len(lpar),len(epar))\n",
    "            continue\n",
    "        sec_alignments = []\n",
    "        for p in range(len(lpar)):\n",
    "            sec_alignments.append(get_sec_alignments(lpar[p],epar[p]))\n",
    "        alignments.append(sec_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "    for s in a:\n",
    "        for p in s:\n",
    "            count += len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6350"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(['Igitur Symmachus scolasticus Iudaeus iubente non rege sed tyranno dictavit praecepta die quarta feria septimo kalend.'],\n",
       "   'Then Symmachus an advocate and a Jew at the order of a tyrant rather than a king announced on an appointed day which was a Wednesday the 26th of August in the fourth indiction under the consulship of Olybrius that on the following Sabbath the Arians would take possession of the Catholic churches.'),\n",
       "  (['Septembr.'],\n",
       "   'But He who does not allow his faithful worshippers to be oppressed by unbelievers soon brought upon Theodoric the same punishment that Arius the founder of his religion had suffered for the king was seized with a diarrhea and after three days of open bowels lost both his throne and his life on the very same day on which he rejoiced to attack the churches.'),\n",
       "  (['indictione quarta Olybrio consule ut die dominico adveniente Arriani basilicas catholicas invaderent.'],\n",
       "   'But before breathing his last he named his grandson Athalaric as his successor.'),\n",
       "  (['Sed qui non patitur fideles cultores suos ab alienigenis opprimi mox intulit in eum sententiam Arrii auctoris religionis eius fluxum ventris incurrit et dum intra triduum evacuatus fuisset eodem die quo se gaudebat ecclesias invadere simul regnum et animam amisit.',\n",
       "    'Ergo antequam exhalaret nepotem suum Athalaricum in regnum constituit.',\n",
       "    'Se autem vivo fecit sibi monimentum ex lapide quadrato mirae magnitudinis opus et saxum ingens quod superponeret inquisivit.'],\n",
       "   'During his lifetime he had made himself a mausoleum of squared blocks of stone a work of extraordinary size and sought out a huge rock to place upon it.')],\n",
       " []]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('aligned_sentences/amm_lat_sentences.json','w') #amm_lat.xml\n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = get_sentences(echaps[0][6],esects[0][6])\n",
    "ls = get_sentences(lchaps[0][6],lsects[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"11\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"12\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"13\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"14\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"15\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"16\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"17\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"18\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"19\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"20\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"11\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"12\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"13\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"14\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"15\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"16\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"17\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"18\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"19\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"20\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"21\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"22\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"23\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"24\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"25\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"26\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"11\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"12\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"13\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"14\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"15\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"16\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"17\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"18\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"19\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"20\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"21\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"11\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"12\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"13\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"14\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"15\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"11\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"12\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"13\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"14\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"15\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"16\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"1\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"2\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"3\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"4\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"5\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"6\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"7\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"8\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"9\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"10\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"11\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"12\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"13\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"14\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"15\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"16\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"17\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"18\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"19\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"20\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"21\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"22\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"23\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"24\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"25\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"26\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"27\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"28\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"29\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"30\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"31\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"32\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"33\" unit=\"section\"></milestone>,\n",
       " <milestone n=\"34\" unit=\"section\"></milestone>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsects[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esects[0][6][0]['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<milestone n=\"1\" unit=\"section\"></milestone>"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(echaps[0][6].children)[3].find('milestone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(lp)):\n",
    "    print(lp[i]['n'],ep[i]['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(lb)):\n",
    "    lp = lb[i].findAll('div2', {'type':'chapter'})\n",
    "    ep = eb[i].findAll('div2', {'type':'chapter'})\n",
    "    print(len(lp),len(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "s = lchaps[2][1].findAll('milestone')[0]\n",
    "te = []\n",
    "t = []\n",
    "i = 1\n",
    "while s.next_sibling != None:\n",
    "    s = s.next_sibling\n",
    "    if s.name == 'milestone':\n",
    "        print(s['n'])\n",
    "        if int(s['n']) == int(lsects[2][1][i][0]['n']):\n",
    "            i += 1\n",
    "            te.append(' '.join(t))\n",
    "            t = []\n",
    "    if s.name in [None,'corr', 'name']:\n",
    "        try:\n",
    "            t.append(s.text.strip())\n",
    "        except:\n",
    "            t.append(s.strip())\n",
    "    #  lchaps[2][1].findAll('milestone')[-1].next_sibling.next_sibling"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
