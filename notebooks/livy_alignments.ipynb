{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(soup):\n",
    "    l = []\n",
    "    for n in soup:\n",
    "        try:\n",
    "            l.append(int(n['n']))\n",
    "        except:\n",
    "            l.append(-1)  \n",
    "    return remove_duplicates(l)\n",
    "\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    i = 0\n",
    "    while i+1 < len(l):\n",
    "        if l[i] == l[i+1]:\n",
    "            l[i+1] += 1\n",
    "        if l[i] > l[i+1] and l[i+1] != -1:\n",
    "            l[i] = l[i+1]-1\n",
    "        i += 1\n",
    "    return l\n",
    "\n",
    "def match(large,small):\n",
    "    l = make_list(large)\n",
    "    s = make_list(small)\n",
    "    li = 0\n",
    "    si = 0\n",
    "    groups = []\n",
    "    #print(l,s)\n",
    "    while si < len(s) and li < len(l):\n",
    "        if s[si] == -1:\n",
    "            si += 1\n",
    "            continue\n",
    "        if l[li] == -1:\n",
    "            li += 1\n",
    "            continue\n",
    "        lgroup = []\n",
    "        sgroup = []\n",
    "        if l[li] == s[si]:\n",
    "            lgroup.append(li)\n",
    "            sgroup.append(si)\n",
    "              \n",
    "        if si+1 < len(s) and li+1 < len(l): \n",
    "            if s[si+1] == -1:\n",
    "                si += 1\n",
    "            \n",
    "            if l[li+1] == -1:\n",
    "                li += 1\n",
    "            \n",
    "            \n",
    "            if l[li+1] != s[si+1]:\n",
    "                while l[li+1] != s[si+1]:\n",
    "                    if si+1 < len(s) and li+1 < len(l):\n",
    "                        if l[li+1] > s[si+1]:\n",
    "                            sgroup.append(si+1)\n",
    "                            si += 1\n",
    "                            if s[si] == -1:\n",
    "                                if si+1 < len(s):\n",
    "                                    si += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                        else:\n",
    "                            lgroup.append(li+1)\n",
    "                            li += 1\n",
    "                            if l[li] == -1:\n",
    "                                if li+1 < len(l):\n",
    "                                    li += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                    else:\n",
    "                        break\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup])\n",
    "            else:\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup]) \n",
    "        else:\n",
    "            break        \n",
    "    \n",
    "    groups.append([sgroup,lgroup])\n",
    "    return groups\n",
    "        \n",
    "\n",
    "def align_soups(m,small,large):\n",
    "    s_alignments = []\n",
    "    l_alignments = []\n",
    "    for g in m:\n",
    "        s = []\n",
    "        l = []\n",
    "        for i in g[0]:\n",
    "            s.append(small[i])\n",
    "        for i in g[1]:\n",
    "            l.append(large[i])\n",
    "        s_alignments.append(s)\n",
    "        l_alignments.append(l)   \n",
    "    return s_alignments,l_alignments\n",
    "\n",
    "\n",
    "def get_pairs(latin_soup, eng_soup, tag, typename, typos):\n",
    "    \"\"\" Given two soups, returns soups of equal size, with each element corresponding to the same\n",
    "        element in the other soup.\n",
    "        Lists may contain further lists of soups which are aligned with one another, they are combined into one list\n",
    "        in the first step of this function.\n",
    "    \"\"\"\n",
    "    lat_sect,eng_sect = [], []\n",
    "    \n",
    "    for i in range(len(latin_soup)):\n",
    "        lat_sect += latin_soup[i].findAll(tag, {typename: typos})\n",
    "    for i in range(len(eng_soup)):\n",
    "        eng_sect += eng_soup[i].findAll(tag, {typename: typos})\n",
    "\n",
    "    if len(lat_sect) > len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].parent['n'])\n",
    "            m = match(lat_sect,eng_sect)\n",
    "            eng_alignments,lat_alignments = align_soups(m,eng_sect,lat_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    elif len(lat_sect) < len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "            m = match(eng_sect,lat_sect)\n",
    "            lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    else:\n",
    "        if eng_sect[0]['n'] == 'intro':\n",
    "            try:\n",
    "                #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "                m = match(eng_sect,lat_sect)\n",
    "                lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "                return lat_alignments,eng_alignments\n",
    "            except:\n",
    "                print(\"error with {}, {}\".format(tag,typos))\n",
    "        else:\n",
    "            \n",
    "            return lat_sect, eng_sect\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def align_pars(lsentences,esentences):\n",
    "    \n",
    "    if lsentences[-1] == '':\n",
    "        lsentences = lsentences[:-1]\n",
    "    if esentences[-1] == '':\n",
    "        esentences = esentences[:-1]\n",
    "        \n",
    "    if len(lsentences) > len(esentences):\n",
    "        alignments = align(lsentences,esentences)\n",
    "        target = esentences\n",
    "        source = lsentences\n",
    "    elif len(lsentences) < len(esentences):\n",
    "        alignments = align(esentences,lsentences)\n",
    "        target = lsentences\n",
    "        source = esentences\n",
    "    else:\n",
    "        return zip(lsentences,esentences)\n",
    "    \n",
    "    sentence_tuples = []\n",
    "    for k in alignments.keys():\n",
    "        sentence_tuples.append((target[k], [source[j] for j in alignments[k]]))\n",
    "        \n",
    "    return sentence_tuples\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "def clean_lemma(word):\n",
    "    try:\n",
    "        word = re.sub('[0-9]+', '', word)\n",
    "        word = re.sub('[-_]+', '', word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_lemmata_defs(latin):\n",
    "    words = lemmatizer.lemmatize(latin)\n",
    "    ldefs = []\n",
    "    unmatched = []\n",
    "\n",
    "    for word in words:\n",
    "        word = clean_lemma(word)\n",
    "        try:\n",
    "            ldefs.append(defs[word]['definition'].split(','))\n",
    "        except:\n",
    "            unmatched.append(word)\n",
    "    return ldefs,unmatched\n",
    "\n",
    "\n",
    "def get_defs(ldefs):\n",
    "    lldefs=[]\n",
    "    for l in ldefs:\n",
    "        lldefs += t(l[0])\n",
    "    return lldefs\n",
    "\n",
    "\n",
    "def get_match_count(english,latin):\n",
    "    \n",
    "    ldefs,unmatched = get_lemmata_defs(latin)\n",
    "    ldefs = get_defs(ldefs)\n",
    "    count = 0\n",
    "    for word in t(english):\n",
    "        if word in string.punctuation:\n",
    "            continue\n",
    "        if word in ldefs and word not in stop:\n",
    "            count += 1\n",
    "        if word in unmatched:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def align(source,target):\n",
    "    \"\"\" Takes a paragraph of sentences, source, and maps each sentence to a sentence in the \n",
    "        target paragraph. \n",
    "        \n",
    "        Returns a dictionary with the target sentence indices as keys, and the sentences in the source paragraph\n",
    "        which correspond to them as values.\n",
    "    \"\"\"\n",
    "    tsize = len(target)\n",
    "    ssize = len(source)\n",
    "    alignments = defaultdict(list)\n",
    "    alignments[0] = [0]\n",
    "    i = 1\n",
    "    j = 1\n",
    "    while tsize < ssize and i < len(source) and j < tsize:\n",
    "        max_count = get_match_count(source[i],target[j])\n",
    "        max_i = j\n",
    "        if j >= tsize-1:\n",
    "            rge = [j-1]\n",
    "        else:\n",
    "            rge = [j-1,j+1]\n",
    "        for r in rge: \n",
    "            c = get_match_count(source[i],target[r])\n",
    "            if c > max_count:\n",
    "                max_count = c\n",
    "                max_i = r\n",
    "\n",
    "        if max_i != j:\n",
    "            ssize -= 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        alignments[max_i].append(i)\n",
    "        i += 1\n",
    "        \n",
    "  \n",
    "    while j < tsize:\n",
    "        alignments[j].append(i)\n",
    "        j += 1\n",
    "        i += 1\n",
    "    alignments[j-1].extend([k for k in range(i,len(source))])\n",
    "    \n",
    "    return alignments\n",
    "\n",
    "def get_sec_alignments(lpar,epar):\n",
    "    if len(lpar) > len(epar):\n",
    "        if len(epar) > 1:\n",
    "            alignments = align(lpar,epar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(lpar[v])\n",
    "                aligned.append((group,epar[k]))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [('. '.join(lpar),epar[0])] \n",
    "            except:\n",
    "                print('error, {}'.format(epar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    elif len(epar) > len(lpar):\n",
    "        if len(lpar) > 1:\n",
    "            alignments = align(epar,lpar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(epar[v])\n",
    "                aligned.append((lpar[k],group))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [(lpar[0],'. '.join(epar))]\n",
    "            except:\n",
    "                print('error, {}'.format(lpar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    else:\n",
    "        aligned = list(zip(lpar,epar))\n",
    "        \n",
    "    return aligned\n",
    "\n",
    "def get_sentences(chap_soup, sect_soup):\n",
    "    t = bs4.element.Tag\n",
    "    sect = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    c = 0\n",
    "    children = list(chap_soup.children)\n",
    "    soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "    c += 1\n",
    "    while type(soup) != t:\n",
    "        soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "        c += 1\n",
    "    i += 1\n",
    "    while c < len(children):\n",
    "        print(sentences)\n",
    "        while soup.next_sibling != None:\n",
    "            soup = soup.next_sibling\n",
    "            if soup.name == 'milestone':\n",
    "                try:\n",
    "                    if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                        i += 1\n",
    "                        sentences.append(' '.join(sect))\n",
    "                        sect = []\n",
    "                except:\n",
    "                    soup = soup.next_sibling\n",
    "                    i += 1\n",
    "            if soup.name in ['corr', 'name','hi','quote','reg', 'l', 'lg', 'placeName']:\n",
    "                    sect.append(soup.text.strip())\n",
    "            elif soup.name == None:\n",
    "                    sect.append(soup.strip())\n",
    "                    \n",
    "        sentences.append(' '.join(sect))\n",
    "        sect = []\n",
    "        try:\n",
    "            soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "            c += 1\n",
    "            while type(soup) != t and c < len(children):\n",
    "                soup = children[c].find('milestone')\n",
    "                c += 1\n",
    "            i += 1\n",
    "        except:\n",
    "            continue\n",
    "    sentences.append(' '.join(sect))\n",
    "    sect = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = wordpunct_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "f = open('../lemmas.json')\n",
    "defs = json.loads(f.read())\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "tokenizer = WordTokenizer('latin')\n",
    "s_tokenizer = TokenizeSentence('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../../Latin/Livy/opensource/livy.foster01-02_lat.xml')\n",
    "ltext = f.read()\n",
    "f.close()\n",
    "f = open('../../Latin/Livy/opensource/livy.foster01-02.xml')\n",
    "etext = f.read()\n",
    "f.close()\n",
    "\n",
    "lat_sections = re.split(r'<div1 type=\"book\" n=\"\\d\\d?\\d?\">', ltext)\n",
    "eng_sections = re.split(r'<div1 type=\"book\" n=\"\\d\\d?\\d?\">', etext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lchaps = []\n",
    "for book in lat_sections[1:]:\n",
    "    sects = re.split(r'<milestone unit=\"chapter\" n=\"\\d\\d?\\d?\"/>', book)[1:]\n",
    "    lchaps.append(sects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echaps = []\n",
    "for book in eng_sections[1:]:\n",
    "    sects = re.split(r'<milestone unit=\"chapter\" n=\"\\d\\d?\\d?\"/>', book)[1:]\n",
    "    echaps.append(sects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(echaps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lchaps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsects = []\n",
    "for chap in lchaps:\n",
    "    for sec in chap:\n",
    "        sects = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', sec)[1:]\n",
    "        lsects.append(sects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esects = []\n",
    "for chap in echaps:\n",
    "    for sec in chap:\n",
    "        sects = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', sec)[1:]\n",
    "        esects.append(sects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' <reg>iam</reg> primum omnium satis constat Troia capta\\n\\t\\t\\t\\t\\tin ceteros saevitum esse Troianos: duobus, Aeneae\\n\\t\\t\\t\\t\\tAntenorique, et vetusti iure hospitii et quia pacis\\n\\t\\t\\t\\t\\treddendaeque Helenae semper auctores fuerunt,\\n\\t\\t\\t\\t\\tomne ius belli Achivos abstinuisse;',\n",
       " ' casibus deinde\\n\\t\\t\\t\\t\\tvariis Antenorem cum multitudine Enetum, qui\\n\\t\\t\\t\\t\\tseditione ex Paphlagonia pulsi et sedes et ducem\\n\\t\\t\\t\\t\\trege Pylaemene ad Troiam amisso quaerebant,\\n\\t\\t\\t\\t\\tvenisse in intimum maris Hadriatici sinum,<milestone unit=\"section\" n=\"3?\"/> <reg>euganeisque</reg>, qui inter mare Alpesque incolebant, pulsis,\\n\\t\\t\\t\\t\\tEnetos Troianosque eas tenuisse terras. <reg>et</reg> in quem\\n\\t\\t\\t\\t\\tprimum egressi sunt locum Troia vocatur, pagoque\\n\\t\\t\\t\\t\\tinde Troiano nomen est: gens universa Veneti\\n\\t\\t\\t\\t\\tappellati.',\n",
       " ' <name>Aeneam</name> ab simili clade domo profugum,\\n\\t\\t\\t\\t\\tsed ad maiora rerum initia ducentibus fatis,) rimo\\n\\t\\t\\t\\t\\tin Macedoniam venisse, inde in <corr sic=\"Siciliamn\">Siciliam</corr> quaerentem\\n\\t\\t\\t\\t\\tsedes delatum, <corr sic=\"al\">ab</corr> Sicilia classe ad Laurentem agrum\\n\\t\\t\\t\\t\\ttenuisse.',\n",
       " ' <name>Troia</name> et huic loco nomen est. <reg>ibi</reg> egressi\\n\\t\\t\\t\\t\\tTroiani, ut quibus ab inmenso prope errore nihil\\n\\t\\t\\t\\t\\tpraeter arma et naves superesset, cum praedam ex\\n\\t\\t\\t\\t\\tagris agerent, Latinus rex Aboriginesque, qui tum\\n\\t\\t\\t\\t\\tea tenebant loca, ad arcendam vim advenarum\\n\\t\\t\\t\\t\\tarmati ex urbe atque agris concurrunt.',\n",
       " ' <reg>duplex</reg> inde\\n\\t\\t\\t\\t\\t<pb id=\"p.10\"/>\\n\\t\\t\\t\\t\\tfama est. <reg>alii</reg> proelio victum Latinum pacem cum\\n\\t\\t\\t\\t\\tAenea, deinde affinitatem iunxisse tradunt:',\n",
       " ' alii, cum\\n\\t\\t\\t\\t\\tinstructae acies constitissent, priusquam signa canerent processisse Latinum inter primores ducemque\\n\\t\\t\\t\\t\\tadvenarum evocasse ad conloquium; percunctatum\\n\\t\\t\\t\\t\\tdeinde qui mortales essent, unde aut quo casu profecti domo quidve quaerentes in agrum Laurentinum<note>Laurentinum <foreign lang=\"greek\">W:</foreign> Laurentem MO2DL <foreign lang=\"greek\">s.</foreign></note> exissent,',\n",
       " ' postquam audierit <corr sic=\"multitudinern\">multitudinem</corr> Troianos esse, ducem Aeneam, filium <corr sic=\"Anclisae\">Anchisae</corr> et\\n\\t\\t\\t\\t\\tVeneris, cremata patria domo profugos sedem condendaeque urbi locum quaerere, et nobilitatem\\n\\t\\t\\t\\t\\tadmiratum gentis virique et animum vel bello vel\\n\\t\\t\\t\\t\\tpaci paratum, dextra data fidem futurae amicitiae\\n\\t\\t\\t\\t\\tsanxisse.',\n",
       " ' <reg>inde</reg> foedus ictum inter duces, inter exercitus salutationem factam; Aeneam apud Latinum\\n\\t\\t\\t\\t\\tfuisse in hospitio; ibi Latinum apud penates deos\\n\\t\\t\\t\\t\\tdomesticum publico adiunxisse foedus filia Aeneae\\n\\t\\t\\t\\t\\tin matrimonium data.',\n",
       " ' <reg>ea</reg> res utique Troianis spem\\n\\t\\t\\t\\t\\tadfirmat tandem stabili certaque sede finiendi erroris.\\n\\t\\t\\t\\t\\t<reg>oppidum</reg> condunt;',\n",
       " ' <name>Aeneas</name> ab nomine uxoris Lavinium appellat. <reg>brevi</reg> stirpis quoque virilis ex novo\\n\\t\\t\\t\\t\\tmatrimonio fuit, cui Ascanium parentes dixere\\n\\t\\t\\t\\t\\tnomen.</p>\\n\\t\\t\\t\\t<p>']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsects[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the sections seem to match up, so we can just proceed as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "\n",
    "for s in eng_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        eng_text.append(soup.text)\n",
    "    else:\n",
    "        eng_text.append(soup.text)\n",
    "\n",
    "lat_text = []\n",
    "\n",
    "for s in lat_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        lat_text.append(soup.text)\n",
    "    else:\n",
    "        lat_text.append(soup.text)\n",
    "\n",
    "eng_text = eng_text[1:]\n",
    "lat_text = lat_text[1:]\n",
    "\n",
    "alignments = []\n",
    "\n",
    "for c in range(len(eng_text)):\n",
    "    es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "    ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "\n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "\n",
    "for t in esects:\n",
    "    text = []\n",
    "    for s in t:\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        if soup.find('note'):\n",
    "            s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "            soup = BeautifulSoup(s,'lxml')\n",
    "            text.append(soup.text)\n",
    "        else:\n",
    "            text.append(soup.text)\n",
    "    eng_text.append(text)\n",
    "\n",
    "lat_text = []\n",
    "\n",
    "for t in lsects:\n",
    "    text = []\n",
    "    for s in t:\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        if soup.find('note'):\n",
    "            s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "            soup = BeautifulSoup(s,'lxml')\n",
    "            text.append(soup.text)\n",
    "        else:\n",
    "            text.append(soup.text)\n",
    "    lat_text.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iam primum omnium satis constat Troia capta\\n\\t\\t\\t\\t\\tin ceteros saevitum esse Troianos: duobus, Aeneae\\n\\t\\t\\t\\t\\tAntenorique, et vetusti iure hospitii et quia pacis\\n\\t\\t\\t\\t\\treddendaeque Helenae semper auctores fuerunt,\\n\\t\\t\\t\\t\\tomne ius belli Achivos abstinuisse;'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_text[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bello deinde Aborigines Troianique simul \\t\\t\\t\\t\\tpetiti.',\n",
       " 'Turnus rex Rutulorum cui pacta Lavinia \\t\\t\\t\\t\\tante adventum Aeneae fuerat praelatum sibi \\t\\t\\t\\t\\t \\t\\t\\t\\t\\tadvenam aegre patiens simul Aeneae Latinoque bellum \\t\\t\\t\\t\\tintulerat.']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-4217f0a927c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlat_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-4217f0a927c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlat_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlat_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\. \\. \\.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "alignments = []\n",
    "\n",
    "\n",
    "for s in range(len(eng_text)):\n",
    "    for c in range(len(eng_text[c])):\n",
    "        try:\n",
    "            es = ' '.join(eng_text[c][s].replace('\\n',' ').split())\n",
    "            ls = lat_text[c][s].replace('\\n', ' ')\n",
    "        except:\n",
    "            print(len(eng_text[c],lat_text[c]))\n",
    "            continue\n",
    "        ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "        es = re.sub(r'\\. \\. \\.','',es)\n",
    "        ls = re.sub('[:;,]','',ls)\n",
    "        es = re.sub('[:;,]','',es)\n",
    "\n",
    "        epar = sent_tokenize(es)\n",
    "        lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "        alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iam primum omnium satis constat Troia capta \\t\\t\\t\\t\\tin ceteros saevitum esse Troianos duobus Aeneae \\t\\t\\t\\t\\tAntenorique et vetusti iure hospitii et quia pacis \\t\\t\\t\\t\\treddendaeque Helenae semper auctores fuerunt \\t\\t\\t\\t\\tomne ius belli Achivos abstinuisse',\n",
       "  'First of all then it is generally agreed that when Troy was taken vengeance was wreaked upon the other Trojans but that two Aeneas and Antenor were spared all the penalties of war by the Achivi owing to long-standing claims of hospitality and because they had always advocated peace and the giving back of Helen.')]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'bello deinde Aborigines Troianique simul\\n\\t\\t\\t\\t\\tpetiti. Turnus, rex Rutulorum, cui pacta Lavinia\\n\\t\\t\\t\\t\\tante adventum Aeneae fuerat, praelatum sibi\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tadvenam aegre patiens simul Aeneae Latinoque bellum\\n\\t\\t\\t\\t\\tintulerat.',\n",
       " 'neutra acies laeta ex eo certamine abiit:\\n\\t\\t\\t\\t\\tvicti Rutuli: victores Aborigines Troianique ducem\\n\\t\\t\\t\\t\\tLatinum amisere.',\n",
       " 'inde Turnus Rutulique diffisi\\n\\t\\t\\t\\t\\trebus ad florentes opes Etruscorum Mezentiumque\\n\\t\\t\\t\\t\\tregem eorum confugiunt, qui Caere opulento tum\\n\\t\\t\\t\\t\\toppido imperitans, iam inde ab initio minime laetus\\n\\t\\t\\t\\t\\tnovae origine urbis, et tum nimio plus quam satis\\n\\t\\t\\t\\t\\ttutum esset accolis rem Troianam crescere ratus,\\n\\t\\t\\t\\t\\thaud gravatim socia arma Rutulis iunxit.',\n",
       " 'Aeneas,\\n\\t\\t\\t\\t\\tadversus tanti belli terrorem ut animos Aboriginum\\n\\t\\t\\t\\t\\tsibi conciliaret, nec sub eodem iure solum sed etiam\\n\\t\\t\\t\\t\\tnomine omnes essent, Latinos utramque gentem\\n\\t\\t\\t\\t\\tappellavit.',\n",
       " 'nec deinde Aborigines Troianis studio\\n\\t\\t\\t\\t\\tac fide erga regem Aeneam cessere. fretusque his\\n\\t\\t\\t\\t\\tanimis coalescentium in dies magis duorum populorum Aeneas, quamquam tanta opibus Etruria erat\\n\\t\\t\\t\\t\\tut iam non terras solum sed mare etiam per totam\\n\\t\\t\\t\\t\\tItaliae longitudinem ab Alpibus ad fretum Siculum\\n\\t\\t\\t\\t\\tfama nominis sui inplesset, tamen, cum moenibus\\n\\t\\t\\t\\t\\tbellum propulsare posset, in aciem copias eduxit.\\n\\t\\t\\t\\t\\t',\n",
       " 'secundum inde proelium Latinis, Aeneae etiam\\n\\t\\t\\t\\t\\tultimum operum mortalium fuit. situs est,\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\tquemcumque eum dici ius fasque est, super Numicum\\n\\t\\t\\t\\t\\tflumen: Iovem indigetem appellant.\\n\\t\\t\\t\\t']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
