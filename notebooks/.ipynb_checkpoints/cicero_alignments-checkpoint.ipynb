{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(soup):\n",
    "    l = []\n",
    "    for n in soup:\n",
    "        try:\n",
    "            l.append(int(n['n']))\n",
    "        except:\n",
    "            l.append(-1)  \n",
    "    return remove_duplicates(l)\n",
    "\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    i = 0\n",
    "    while i+1 < len(l):\n",
    "        if l[i] == l[i+1]:\n",
    "            l[i+1] += 1\n",
    "        if l[i] > l[i+1] and l[i+1] != -1:\n",
    "            l[i] = l[i+1]-1\n",
    "        i += 1\n",
    "    return l\n",
    "\n",
    "def match(large,small):\n",
    "    l = make_list(large)\n",
    "    s = make_list(small)\n",
    "    li = 0\n",
    "    si = 0\n",
    "    groups = []\n",
    "    #print(l,s)\n",
    "    while si < len(s) and li < len(l):\n",
    "        if s[si] == -1:\n",
    "            si += 1\n",
    "            continue\n",
    "        if l[li] == -1:\n",
    "            li += 1\n",
    "            continue\n",
    "        lgroup = []\n",
    "        sgroup = []\n",
    "        if l[li] == s[si]:\n",
    "            lgroup.append(li)\n",
    "            sgroup.append(si)\n",
    "              \n",
    "        if si+1 < len(s) and li+1 < len(l): \n",
    "            if s[si+1] == -1:\n",
    "                si += 1\n",
    "            \n",
    "            if l[li+1] == -1:\n",
    "                li += 1\n",
    "            \n",
    "            \n",
    "            if l[li+1] != s[si+1]:\n",
    "                while l[li+1] != s[si+1]:\n",
    "                    if si+1 < len(s) and li+1 < len(l):\n",
    "                        if l[li+1] > s[si+1]:\n",
    "                            sgroup.append(si+1)\n",
    "                            si += 1\n",
    "                            if s[si] == -1:\n",
    "                                if si+1 < len(s):\n",
    "                                    si += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                        else:\n",
    "                            lgroup.append(li+1)\n",
    "                            li += 1\n",
    "                            if l[li] == -1:\n",
    "                                if li+1 < len(l):\n",
    "                                    li += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                    else:\n",
    "                        break\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup])\n",
    "            else:\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup]) \n",
    "        else:\n",
    "            break        \n",
    "    \n",
    "    groups.append([sgroup,lgroup])\n",
    "    return groups\n",
    "        \n",
    "\n",
    "def align_soups(m,small,large):\n",
    "    s_alignments = []\n",
    "    l_alignments = []\n",
    "    for g in m:\n",
    "        s = []\n",
    "        l = []\n",
    "        for i in g[0]:\n",
    "            s.append(small[i])\n",
    "        for i in g[1]:\n",
    "            l.append(large[i])\n",
    "        s_alignments.append(s)\n",
    "        l_alignments.append(l)   \n",
    "    return s_alignments,l_alignments\n",
    "\n",
    "\n",
    "def get_pairs(latin_soup, eng_soup, tag, typename, typos):\n",
    "    \"\"\" Given two soups, returns soups of equal size, with each element corresponding to the same\n",
    "        element in the other soup.\n",
    "        Lists may contain further lists of soups which are aligned with one another, they are combined into one list\n",
    "        in the first step of this function.\n",
    "    \"\"\"\n",
    "    lat_sect,eng_sect = [], []\n",
    "    \n",
    "    for i in range(len(latin_soup)):\n",
    "        lat_sect += latin_soup[i].findAll(tag, {typename: typos})\n",
    "    for i in range(len(eng_soup)):\n",
    "        eng_sect += eng_soup[i].findAll(tag, {typename: typos})\n",
    "\n",
    "    if len(lat_sect) > len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].parent['n'])\n",
    "            m = match(lat_sect,eng_sect)\n",
    "            eng_alignments,lat_alignments = align_soups(m,eng_sect,lat_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    elif len(lat_sect) < len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "            m = match(eng_sect,lat_sect)\n",
    "            lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    else:\n",
    "        if eng_sect[0]['n'] == 'intro':\n",
    "            try:\n",
    "                #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "                m = match(eng_sect,lat_sect)\n",
    "                lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "                return lat_alignments,eng_alignments\n",
    "            except:\n",
    "                print(\"error with {}, {}\".format(tag,typos))\n",
    "        else:\n",
    "            \n",
    "            return lat_sect, eng_sect\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def align_pars(lsentences,esentences):\n",
    "    \n",
    "    if lsentences[-1] == '':\n",
    "        lsentences = lsentences[:-1]\n",
    "    if esentences[-1] == '':\n",
    "        esentences = esentences[:-1]\n",
    "        \n",
    "    if len(lsentences) > len(esentences):\n",
    "        alignments = align(lsentences,esentences)\n",
    "        target = esentences\n",
    "        source = lsentences\n",
    "    elif len(lsentences) < len(esentences):\n",
    "        alignments = align(esentences,lsentences)\n",
    "        target = lsentences\n",
    "        source = esentences\n",
    "    else:\n",
    "        return zip(lsentences,esentences)\n",
    "    \n",
    "    sentence_tuples = []\n",
    "    for k in alignments.keys():\n",
    "        sentence_tuples.append((target[k], [source[j] for j in alignments[k]]))\n",
    "        \n",
    "    return sentence_tuples\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "def clean_lemma(word):\n",
    "    try:\n",
    "        word = re.sub('[0-9]+', '', word)\n",
    "        word = re.sub('[-_]+', '', word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_lemmata_defs(latin):\n",
    "    words = lemmatizer.lemmatize(latin)\n",
    "    ldefs = []\n",
    "    unmatched = []\n",
    "\n",
    "    for word in words:\n",
    "        word = clean_lemma(word)\n",
    "        try:\n",
    "            ldefs.append(defs[word]['definition'].split(','))\n",
    "        except:\n",
    "            unmatched.append(word)\n",
    "    return ldefs,unmatched\n",
    "\n",
    "\n",
    "def get_defs(ldefs):\n",
    "    lldefs=[]\n",
    "    for l in ldefs:\n",
    "        lldefs += t(l[0])\n",
    "    return lldefs\n",
    "\n",
    "\n",
    "def get_match_count(english,latin):\n",
    "    \n",
    "    ldefs,unmatched = get_lemmata_defs(latin)\n",
    "    ldefs = get_defs(ldefs)\n",
    "    count = 0\n",
    "    for word in t(english):\n",
    "        if word in string.punctuation:\n",
    "            continue\n",
    "        if word in ldefs and word not in stop:\n",
    "            count += 1\n",
    "        if word in unmatched:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def align(source,target):\n",
    "    \"\"\" Takes a paragraph of sentences, source, and maps each sentence to a sentence in the \n",
    "        target paragraph. \n",
    "        \n",
    "        Returns a dictionary with the target sentence indices as keys, and the sentences in the source paragraph\n",
    "        which correspond to them as values.\n",
    "    \"\"\"\n",
    "    tsize = len(target)\n",
    "    ssize = len(source)\n",
    "    alignments = defaultdict(list)\n",
    "    alignments[0] = [0]\n",
    "    i = 1\n",
    "    j = 1\n",
    "    while tsize < ssize and i < len(source) and j < tsize:\n",
    "        max_count = get_match_count(source[i],target[j])\n",
    "        max_i = j\n",
    "        if j >= tsize-1:\n",
    "            rge = [j-1]\n",
    "        else:\n",
    "            rge = [j-1,j+1]\n",
    "        for r in rge: \n",
    "            c = get_match_count(source[i],target[r])\n",
    "            if c > max_count:\n",
    "                max_count = c\n",
    "                max_i = r\n",
    "\n",
    "        if max_i != j:\n",
    "            ssize -= 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        alignments[max_i].append(i)\n",
    "        i += 1\n",
    "        \n",
    "  \n",
    "    while j < tsize:\n",
    "        alignments[j].append(i)\n",
    "        j += 1\n",
    "        i += 1\n",
    "    alignments[j-1].extend([k for k in range(i,len(source))])\n",
    "    \n",
    "    return alignments\n",
    "\n",
    "def get_sec_alignments(lpar,epar):\n",
    "    if len(lpar) > len(epar):\n",
    "        if len(epar) > 1:\n",
    "            alignments = align(lpar,epar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(lpar[v])\n",
    "                aligned.append((group,epar[k]))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [('. '.join(lpar),epar[0])] \n",
    "            except:\n",
    "                print('error, {}'.format(epar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    elif len(epar) > len(lpar):\n",
    "        if len(lpar) > 1:\n",
    "            alignments = align(epar,lpar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(epar[v])\n",
    "                aligned.append((lpar[k],group))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [(lpar[0],'. '.join(epar))]\n",
    "            except:\n",
    "                print('error, {}'.format(lpar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    else:\n",
    "        aligned = list(zip(lpar,epar))\n",
    "        \n",
    "    return aligned\n",
    "\n",
    "def get_sentences(chap_soup, sect_soup):\n",
    "    t = bs4.element.Tag\n",
    "    sect = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    c = 0\n",
    "    children = list(chap_soup.children)\n",
    "    soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "    c += 1\n",
    "    while type(soup) != t:\n",
    "        soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "        c += 1\n",
    "    i += 1\n",
    "    while c < len(children):\n",
    "        print(sentences)\n",
    "        while soup.next_sibling != None:\n",
    "            soup = soup.next_sibling\n",
    "            if soup.name == 'milestone':\n",
    "                try:\n",
    "                    if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                        i += 1\n",
    "                        sentences.append(' '.join(sect))\n",
    "                        sect = []\n",
    "                except:\n",
    "                    soup = soup.next_sibling\n",
    "                    i += 1\n",
    "            if soup.name in ['corr', 'name','hi','quote','reg', 'l', 'lg', 'placeName']:\n",
    "                    sect.append(soup.text.strip())\n",
    "            elif soup.name == None:\n",
    "                    sect.append(soup.strip())\n",
    "                    \n",
    "        sentences.append(' '.join(sect))\n",
    "        sect = []\n",
    "        try:\n",
    "            soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "            c += 1\n",
    "            while type(soup) != t and c < len(children):\n",
    "                soup = children[c].find('milestone')\n",
    "                c += 1\n",
    "            i += 1\n",
    "        except:\n",
    "            continue\n",
    "    sentences.append(' '.join(sect))\n",
    "    sect = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = wordpunct_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "f = open('../lemmas.json')\n",
    "defs = json.loads(f.read())\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "tokenizer = WordTokenizer('latin')\n",
    "s_tokenizer = TokenizeSentence('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../../Latin/Cicero/opensource/cic.amic_falc_lat.xml')\n",
    "ltext = f.read()\n",
    "f.close()\n",
    "f = open('../../Latin/Cicero/opensource/cic.amic_falc_eng.xml')\n",
    "etext = f.read()\n",
    "f.close()\n",
    "\n",
    "lat_sections = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', ltext)\n",
    "\n",
    "eng_sections = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', etext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lat_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "\n",
    "for s in eng_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        eng_text.append(soup.text)\n",
    "    else:\n",
    "        eng_text.append(soup.text)\n",
    "\n",
    "lat_text = []\n",
    "\n",
    "for s in lat_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        lat_text.append(soup.text)\n",
    "    else:\n",
    "        lat_text.append(soup.text)\n",
    "\n",
    "eng_text = eng_text[1:]\n",
    "lat_text = lat_text[1:]\n",
    "\n",
    "alignments = []\n",
    "\n",
    "for c in range(len(eng_text)):\n",
    "    es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "    ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "\n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "    for s in a:\n",
    "        count += len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('aligned_sentences/cicero_lael_friendship_sentences.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../../Latin/Cicero/opensource/cic.div_falc_lat.xml')\n",
    "ltext = f.read()\n",
    "f.close()\n",
    "f = open('../../Latin/Cicero/opensource/cic.div_falc_eng.xml')\n",
    "etext = f.read()\n",
    "f.close()\n",
    "\n",
    "lat_sections = re.split(r'<milestone unit=\"chapter\" n=\"\\d\\d?\\d?\"/>', ltext)\n",
    "eng_sections = re.split(r'<milestone unit=\"chapter\" n=\"\\d\\d?\\d?\"/>', etext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsecs = []\n",
    "for sec in lat_sections:\n",
    "    ls = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', sec)\n",
    "    lsecs.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esecs = []\n",
    "for sec in eng_sections:\n",
    "    es = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', sec)\n",
    "    esecs.append(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(sec):\n",
    "    soup = BeautifulSoup(sec,'lxml')\n",
    "    if soup.find('note'):\n",
    "        sec = re.sub(r'<note>.*</note>', \" \", sec,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(sec,'lxml')\n",
    "        return soup.text\n",
    "    else:\n",
    "        return soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "eng_text = []\n",
    "lat_text = []\n",
    "\n",
    "for i in range(len(lsecs)):\n",
    "    while '' in lsecs[i]:\n",
    "        lsecs[i].remove('')\n",
    "    while '' in esecs[i]:\n",
    "        esecs[i].remove('')        \n",
    "\n",
    "    if len(lsecs[i]) == len(esecs[i]):\n",
    "        for se in range(len(lsecs[i])):\n",
    "            eng_text.append(get_text(esecs[i][se]))\n",
    "            lat_text.append(get_text(lsecs[i][se]))\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_text = eng_text[1:]\n",
    "lat_text = lat_text[1:]\n",
    "epars = []\n",
    "lpars = []\n",
    "\n",
    "alignments = []\n",
    "\n",
    "for c in range(len(eng_text)):\n",
    "    es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "    ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "\n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    \n",
    "    epars.append(epar)\n",
    "    lpars.append(lpar)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "        count += len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1229"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../aligned_sentences/cicero_div_sentences.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../../Latin/Cicero/opensource/cic.off_lat.xml')\n",
    "ltext = f.read()\n",
    "f.close()\n",
    "f = open('../../Latin/Cicero/opensource/cic.off_eng.xml')\n",
    "etext = f.read()\n",
    "f.close()\n",
    "\n",
    "ltext = ''.join(re.split(r'&(.)acute;',ltext)) # gets rid of accented characters \n",
    "etext = ''.join(re.split(r'&(.)acute;',etext))\n",
    "\n",
    "lat_sections = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', ltext)\n",
    "eng_sections = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', etext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lat_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "\n",
    "for s in eng_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        eng_text.append(soup.text)\n",
    "    else:\n",
    "        eng_text.append(soup.text)\n",
    "\n",
    "lat_text = []\n",
    "\n",
    "for s in lat_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        lat_text.append(soup.text)\n",
    "    else:\n",
    "        lat_text.append(soup.text)\n",
    "\n",
    "eng_text = eng_text[1:]\n",
    "lat_text = lat_text[1:]\n",
    "\n",
    "alignments = []\n",
    "\n",
    "for c in range(len(eng_text)):\n",
    "    es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "    ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "\n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "        count += len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../aligned_sentences/cicero_off_sentences.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../../Latin/Cicero/opensource/cic.sen_falc_lat.xml')\n",
    "ltext = f.read()\n",
    "f.close()\n",
    "f = open('../../Latin/Cicero/opensource/cic.sen_falc_eng.xml')\n",
    "etext = f.read()\n",
    "f.close()\n",
    "\n",
    "ltext = ''.join(re.split(r'&(.)acute;',ltext)) # gets rid of accented characters \n",
    "etext = ''.join(re.split(r'&(.)acute;',etext))\n",
    "\n",
    "lat_sections = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', ltext)\n",
    "eng_sections = re.split(r'<milestone unit=\"section\" n=\"\\d\\d?\\d?\"/>', etext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "\n",
    "for s in eng_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        eng_text.append(soup.text)\n",
    "    else:\n",
    "        eng_text.append(soup.text)\n",
    "\n",
    "lat_text = []\n",
    "\n",
    "for s in lat_sections:\n",
    "    soup = BeautifulSoup(s,'lxml')\n",
    "    if soup.find('note'):\n",
    "        s = re.sub(r'<note>.*</note>', \" \", s,flags=re.DOTALL)\n",
    "        soup = BeautifulSoup(s,'lxml')\n",
    "        lat_text.append(soup.text)\n",
    "    else:\n",
    "        lat_text.append(soup.text)\n",
    "\n",
    "eng_text = eng_text[1:]\n",
    "lat_text = lat_text[1:]\n",
    "\n",
    "alignments = []\n",
    "\n",
    "for c in range(len(eng_text)):\n",
    "    es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "    ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "    ls = re.sub('!','.',ls)\n",
    "    es = re.sub('!','.',es)\n",
    "\n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "        count += len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../aligned_sentences/cicero_sen_falc_sentences.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
