{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from nltk.translate import AlignedSent, Alignment,IBMModel1,IBMModel2,IBMModel3,PhraseTable\n",
    "from nltk.translate.phrase_based import phrase_extraction,extract\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.translate.ibm_model import AlignmentInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_tokenizer = WordTokenizer('latin').tokenize\n",
    "j = JVReplacer()\n",
    "eng_tokenizer = wordpunct_tokenize\n",
    "lem = LemmaReplacer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sent,tok):\n",
    "    tokens = []\n",
    "    if type(sent) == list:\n",
    "        for s in sent:\n",
    "            s = re.sub(r'[{}]'.format(string.punctuation),'',s).lower()\n",
    "            s = re.sub(r'  *', ' ', s)\n",
    "            tokens += tok(s.lower())\n",
    "    else:\n",
    "        sent = re.sub(r'[{}]'.format(string.punctuation),'',sent).lower()\n",
    "        sent = re.sub(r'  *', ' ', sent)\n",
    "        tokens += tok(sent)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../aligned_sentences/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caes_bc_sentences.json',\n",
       " 'cicero_lael_friendship_sentences.json',\n",
       " 'vitruvius_sentences.json',\n",
       " 'lacus_no_error_sentences.json',\n",
       " 'catullus.json',\n",
       " 'suetonius_sentences.json',\n",
       " 'cicero_div_sentences.json',\n",
       " 'celsus_sentences.json',\n",
       " 'amm_lat_sentences.json',\n",
       " 'cicero_off_sentences.json',\n",
       " 'caes_bg_sentences_03457.json',\n",
       " 'cicero_sen_falc_sentences.json',\n",
       " 'lacus_fixed_error_sentences.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bitext = []\n",
    "\n",
    "for f in os.listdir():\n",
    "    if f == 'amm_lat_sentences.json':\n",
    "        continue\n",
    "    of = open(f)\n",
    "    aligned_sentences = json.loads(of.read())\n",
    "    of.close()\n",
    "\n",
    "    for a in aligned_sentences:\n",
    "        for s in a:\n",
    "            lat_sent = [j.replace(t) for t in tokenize_sentences(s[0],lat_tokenizer)]\n",
    "            eng_sent = tokenize_sentences(s[1], eng_tokenizer)\n",
    "            if len(lat_sent) == 0 or len(eng_sent) == 0:\n",
    "                continue\n",
    "            bitext.append(AlignedSent(lat_sent,eng_sent))\n",
    "\n",
    "of = open('amm_lat_sentences.json')\n",
    "aligned_sentences = json.loads(of.read())\n",
    "of.close()\n",
    "\n",
    "for a in aligned_sentences:\n",
    "    for p in a:\n",
    "        for s in p:\n",
    "            lat_sent = [j.replace(t) for t in tokenize_sentences(s[0],lat_tokenizer)]\n",
    "            #while '' in lat_sent:\n",
    "            #    lat_sent.remove('')\n",
    "            #lat_sent = lem.lemmatize(lat_sent)\n",
    "            eng_sent = tokenize_sentences(s[1], eng_tokenizer)\n",
    "            if len(lat_sent) == 0 or len(eng_sent) == 0:\n",
    "                continue\n",
    "            bitext.append(AlignedSent(lat_sent,eng_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21953"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bitext[3].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_bitext = []\n",
    "\n",
    "for f in os.listdir():\n",
    "    if f == 'amm_lat_sentences.json':\n",
    "        continue\n",
    "    of = open(f)\n",
    "    aligned_sentences = json.loads(of.read())\n",
    "    of.close()\n",
    "\n",
    "    for a in aligned_sentences:\n",
    "        for s in a:\n",
    "            lat_sent = [j.replace(t) for t in tokenize_sentences(s[0],lat_tokenizer)]\n",
    "            eng_sent = tokenize_sentences(s[1], eng_tokenizer)\n",
    "            if len(lat_sent) == 0 or len(eng_sent) == 0:\n",
    "                continue\n",
    "            eng_bitext.append(AlignedSent(eng_sent, lat_sent))\n",
    "\n",
    "of = open('amm_lat_sentences.json')\n",
    "aligned_sentences = json.loads(of.read())\n",
    "of.close()\n",
    "\n",
    "for a in aligned_sentences:\n",
    "    for p in a:\n",
    "        for s in p:\n",
    "            lat_sent = [j.replace(t) for t in tokenize_sentences(s[0],lat_tokenizer)]\n",
    "            #while '' in lat_sent:\n",
    "            #    lat_sent.remove('')\n",
    "            #lat_sent = lem.lemmatize(lat_sent)\n",
    "            eng_sent = tokenize_sentences(s[1], eng_tokenizer)\n",
    "            if len(lat_sent) == 0 or len(eng_sent) == 0:\n",
    "                continue\n",
    "            eng_bitext.append(AlignedSent(eng_sent,lat_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat_mod = IBMModel2(bitext,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_mod = IBMModel2(eng_bitext,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.translate.ibm_model.AlignmentInfo at 0x7f6ff8812d30>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_mod.best_model2_alignment(AlignedSent(word,mots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ebitext = eng_bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbitext = bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = lbitext[2].words\n",
    "mots = lbitext[2].mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 1, 2, 9, 8, 27, 10, 7, 40, 14, 20, 3, 22, 40, 40, 29, 16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.trg_sentence, a.src_sentence = a.src_sentence, a.trg_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-12"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_mod.prob_t_a_given_s(alignment_info=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for b in range(len(ebitext)):\n",
    "    if ebitext[b].mots[-1] != 'NULL':\n",
    "        ebitext[b].mots.append('NULL')\n",
    "    if ebitext[b].words[-1] != 'NULL':\n",
    "        ebitext[b].words.append('NULL')\n",
    "    lens = [len(ebitext[b].words),len(ebitext[b].mots)]\n",
    "\n",
    "    alin = [list(tup) for tup in ebitext[b].alignment]\n",
    "\n",
    "    for i,a in enumerate(alin):\n",
    "        if None in a:\n",
    "            alin[i][alin[i].index(None)] = lens[a.index(None)]-1\n",
    "\n",
    "    alin = [tuple(tup) for tup in alin]\n",
    "\n",
    "    ebitext[b].alignment = Alignment(alin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for b in range(len(lbitext)):\n",
    "    if lbitext[b].mots[-1] != 'NULL':\n",
    "        lbitext[b].mots.append('NULL')\n",
    "    if lbitext[b].words[-1] != 'NULL':\n",
    "        lbitext[b].words.append('NULL')\n",
    "    lens = [len(lbitext[b].words),len(lbitext[b].mots)]\n",
    "\n",
    "    alin = [list(tup) for tup in lbitext[b].alignment]\n",
    "\n",
    "    for i,a in enumerate(alin):\n",
    "        if None in a:\n",
    "            alin[i][alin[i].index(None)] = lens[a.index(None)]-1\n",
    "\n",
    "    alin = [tuple(tup) for tup in alin]\n",
    "\n",
    "    lbitext[b].alignment = Alignment(alin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n"
     ]
    }
   ],
   "source": [
    "ephrases = defaultdict(lambda: [0,defaultdict(int)])\n",
    "skip = [1349,9230]\n",
    "base = .5\n",
    "for b in range(len(ebitext)): \n",
    "    if b in skip:\n",
    "        continue\n",
    "    if b == base*1000:\n",
    "        print(b)\n",
    "        base += .5\n",
    "    \n",
    "    pe = phrase_extraction(' '.join(ebitext[b].words), ' '.join(ebitext[b].mots), ebitext[b].alignment,max_phrase_length=4)\n",
    "    pe = list(pe)\n",
    "    for phrase in pe:\n",
    "        english_tokens = eng_tokenizer(phrase[2])\n",
    "        lat_tokens = lat_tokenizer(phrase[3])\n",
    "        \n",
    "        if len(english_tokens) < 5 and len(lat_tokens) < 5:\n",
    "            ephrases[tuple(english_tokens)][1][tuple(lat_tokens)] += 1\n",
    "            ephrases[tuple(english_tokens)][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7997"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n"
     ]
    }
   ],
   "source": [
    "skip = [7997]\n",
    "\n",
    "lphrases = defaultdict(lambda: [0,defaultdict(int)])\n",
    "base = .5\n",
    "for b in range(len(lbitext)): \n",
    "    if b in skip:\n",
    "        continue\n",
    "    if b == base*1000:\n",
    "        print(b)\n",
    "        base += .5\n",
    "    pe = phrase_extraction(' '.join(lbitext[b].words), ' '.join(lbitext[b].mots), lbitext[b].alignment,max_phrase_length=4)    \n",
    "    pe = list(pe)\n",
    "    for phrase in pe:\n",
    "        english_tokens = eng_tokenizer(phrase[3])\n",
    "        lat_tokens = lat_tokenizer(phrase[2])\n",
    "        \n",
    "        if len(english_tokens) < 5 and len(lat_tokens) < 5:\n",
    "            lphrases[tuple(lat_tokens)][1][tuple(english_tokens)] += 1\n",
    "            lphrases[tuple(lat_tokens)][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29866"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ephrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = defaultdict(lambda: [0,defaultdict(int)])\n",
    "untranslated = dict()\n",
    "\n",
    "for phrase in list(ephrases.keys()):\n",
    "    all_added = True\n",
    "    for trg_phrase in ephrases[phrase][1]:\n",
    "        if lphrases[trg_phrase][0] != 0:\n",
    "            for sub_phrase in lphrases[trg_phrase][1]:\n",
    "                if sub_phrase == phrase:\n",
    "                    phrases[phrase][1][trg_phrase] += ephrases[phrase][1][trg_phrase]\n",
    "                    phrases[phrase][0] += ephrases[phrase][1][trg_phrase]\n",
    "                else:\n",
    "                    all_added = False\n",
    "    if not all_added:\n",
    "        untranslated[phrase] = ephrases[phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in untranslated:  # while there are still untranslated phrases\n",
    "    maximum = 0\n",
    "    for possible_translation in untranslated[k][1]:\n",
    "        if untranslated[k][1][possible_translation] > maximum:\n",
    "            maximum = untranslated[k][1][possible_translation]\n",
    "            translation = possible_translation\n",
    "    phrases[k][0] = maximum\n",
    "    phrases[k][1][translation] = maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phrase_table = PhraseTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in phrases:\n",
    "    for t in phrases[k][1]:\n",
    "        phrase_table.add(k,t,log(phrases[k][1][t]/phrases[k][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../pkls/latin_lang_model.pk', 'rb') as fin:\n",
    "    lang_model = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.stack_decoder import StackDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_prob(phrase):\n",
    "    phrase = list(phrase)\n",
    "    if phrase == ['NULL']:\n",
    "        return 1\n",
    "\n",
    "    while 'NULL' in phrase and phrase != []:\n",
    "        phrase.remove('NULL')\n",
    "\n",
    "    if phrase == []:\n",
    "        return 1\n",
    "    product = 1\n",
    "    for i in range(len(phrase)-2):\n",
    "        product *= lang_model.S(phrase[i+2],(phrase[i],phrase[i+1]))\n",
    "    return log(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_model = type('',(object,),{'probability_change': lambda self, context, phrase: language_prob(phrase), 'probability': lambda self, phrase: language_prob(phrase)})()\n",
    "SD = StackDecoder(phrase_table=phrase_table,language_model=language_model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../pkls/translation_model_1.pk', 'wb') as fout:\n",
    "    pickle.dump(SD,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'homines', 'dare', 'flores', 'ad', 'coniugibus']"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','men','give','flowers','to','their','wives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'canis', 'est', 'hic']"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','dog','is','here'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'uia', 'est', 'obstructis']"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','road','is','blocked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caesar', 'est', 'uiuus']"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['caesar','is','alive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nulla', 'est', 'uia']"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['there','is','no','way'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ipse', 'existimauit', 'ita']"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['he','himself','thought','so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ipse', 'ait', 'ita']"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['he','himself','says','so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dixi', 'ita']"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['i','have','said','so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'uir', 'habet', 'tela']"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','man','has','weapons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'uia', 'periculosum', 'est']"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','road','is','dangerous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qui', 'sunt', 'eos']"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['who','are','they'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qui', 'es']"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['who','are','you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ubi', 'es']"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['where','are','you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ubi', 'est']"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['where','is','it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibi', 'erant', 'tres', 'homines', 'hic']"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['there','were','three','men','here'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['est', 'hostis', 'in', 'nostris', 'urbem']"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['there','is','an','enemy','in','our','city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hostis', 'in', 'nostris', 'urbem']"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['an','enemy','in','our','city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hostis', 'apud', 'NULL', 'portam']"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['an','enemy','at','the','gate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hostis']"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['an','enemy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nostri', 'domum', 'est', 'longe']"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['our','house','is','far'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'ultimum', 'tempus']"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','last','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NULL', 'persona', 'non', 'est', 'boni']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['the','person','is','not','good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', 'amicum']"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['to','a','friend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['puto', 'ita']"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['i','think','so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es', 'certa']"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['are','you','sure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sum', 'non', 'miser']"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['i','am','not','unhappy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['es', 'magna', 'persona']"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SD.translate(['you','are','a','great','person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SD.stack_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SD.distortion_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhraseTableEntry(trg_phrase=('amor',), log_prob=0.0),\n",
       " PhraseTableEntry(trg_phrase=('amorem',), log_prob=-0.095310179804324893),\n",
       " PhraseTableEntry(trg_phrase=('amore',), log_prob=-0.20067069546215111),\n",
       " PhraseTableEntry(trg_phrase=('amare',), log_prob=-0.31845373111853459),\n",
       " PhraseTableEntry(trg_phrase=('amoris',), log_prob=-1.2992829841302609),\n",
       " PhraseTableEntry(trg_phrase=('diligere',), log_prob=-1.2992829841302609),\n",
       " PhraseTableEntry(trg_phrase=('amorem', 'NULL'), log_prob=-2.3978952727983707)]"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_table.translations_for(('love',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
