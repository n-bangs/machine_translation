{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all the Latin text, clean it and tokenize it, then write to new file in machine translation directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.tokenize.sentence import TokenizeSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = TokenizeSentence('latin').tokenize\n",
    "wt = WordTokenizer('latin')\n",
    "j = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('latin_text_latin_library/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for directory in os.listdir()[2:]:\n",
    "    dname = '/Users/nickybangs/ds/metis/machine_translation/latin_corpus_tokenized/{}'.format(directory)\n",
    "    os.mkdir(dname)\n",
    "    for file in os.listdir(directory):\n",
    "        fname = file.split('.txt')[0]\n",
    "        fname = dname + '/' + fname + '_tokens.json'\n",
    "        f = open(directory + '/' + file)\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "        text = re.sub(r'\\t', ' ',text).replace('Â´','')\n",
    "        text = re.sub(r'  *', ' ', text)\n",
    "        text = re.sub(r'[0-9]+\\.?','',text)\n",
    "        text = re.sub(r'\\[.\\]','',text)\n",
    "        text = j.replace(text.lower())\n",
    "        text = text.splitlines()\n",
    "\n",
    "        text = text[1:] + text[:-6]\n",
    "\n",
    "        while '' in text:\n",
    "            text.remove('')\n",
    "        while ' ' in text:\n",
    "            text.remove(' ')\n",
    "\n",
    "        sentence_tokens = []\n",
    "\n",
    "        for line in text:\n",
    "            line = line.replace('\\n', ' ')\n",
    "            line_tokens = tok(line)\n",
    "            for token in line_tokens:\n",
    "                token = re.sub(r'[{}]'.format(string.punctuation), ' ', token).strip()\n",
    "                token = re.sub(r'  *',' ',token)\n",
    "                token = wt.tokenize(token)\n",
    "                if len(token) < 3:\n",
    "                    continue\n",
    "                else:\n",
    "                    sentence_tokens.append(token)\n",
    "        f = open(fname,'w')\n",
    "        f.write(json.dumps(sentence_tokens))\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
