{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stop.latin.stops import STOPS_LIST\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "tok = TokenizeSentence('latin').tokenize\n",
    "stop = STOPS_LIST\n",
    "wt = WordTokenizer('latin')\n",
    "j = JVReplacer()\n",
    "lemmatizer = LemmaReplacer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Stupid_Back_Off:\n",
    "    \n",
    "    \n",
    "    def __init__(self,sentence_iterable):\n",
    "        \n",
    "        self.bi_gram = defaultdict(lambda: [0,defaultdict(int)])\n",
    "        self.uni_gram = defaultdict(lambda: [0,defaultdict(int)])\n",
    "        self.no_gram = [0,defaultdict(int)]\n",
    "        self.grams = [self.no_gram,self.uni_gram,self.bi_gram]\n",
    "        \n",
    "        for sentence in sentence_iterable:\n",
    "            sent = ['**','**']+sentence+['**STOP**']\n",
    "            self._update_bigram(sent)\n",
    "            self._update_unigram(sent)\n",
    "            self._update_nogram(sent)\n",
    "            \n",
    " \n",
    "    def _update_nogram(self,sent):\n",
    "        for i in range(len(sent)):\n",
    "            self.no_gram[1][(sent[i])] += 1\n",
    "            self.no_gram[0] += 1\n",
    "\n",
    "    def _update_unigram(self,sent):\n",
    "        for i in range(len(sent)-1):\n",
    "            self.uni_gram[(sent[i])][1][sent[i+1]] += 1\n",
    "            self.uni_gram[(sent[i])][0] += 1\n",
    "            \n",
    "    def _update_bigram(self,sent):\n",
    "        for i in range(len(sent)-2):\n",
    "            self.bi_gram[(sent[i],sent[i+1])][1][sent[i+2]] += 1\n",
    "            self.bi_gram[(sent[i],sent[i+1])][0] += 1\n",
    "            \n",
    "            \n",
    "    def word_proba(self,word,prev=(None),n=2):\n",
    "        if n == 1 or n == 2:\n",
    "            return self.grams[n][prev][1][word] / self.grams[n][prev][0]\n",
    "        else:\n",
    "            return self.grams[0][1][word] / self.grams[0][0]\n",
    "        \n",
    "    def S(self,word,prev_words,i=2):\n",
    "        \"\"\"\n",
    "        Stupid Back-Off model for trigrams.\n",
    "        P(word | prev_words)\n",
    "        \"\"\"\n",
    "        if i == 0:\n",
    "            return self.grams[0][1][word] / self.grams[0][0]\n",
    "\n",
    "        if self.grams[i][prev_words][1][word] > 0:\n",
    "            return self.grams[i][prev_words][1][word] / self.grams[i][prev_words][0]\n",
    "        else:\n",
    "            return .4*self.S(word,prev_words[1:],i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentencesIterator(object):\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        \n",
    "        for dname in os.listdir(self.dirname):\n",
    "            dname = self.dirname + dname\n",
    "            for fname in os.listdir(dname):\n",
    "                fname = dname + '/' + fname\n",
    "                sentences = json.loads(open(fname).read())\n",
    "                for sentence in sentences:\n",
    "                    yield sentence\n",
    "\n",
    "sentences = SentencesIterator('/home/nbangs/Notebooks/machine_translation/latin_corpus_tokenized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb = Stupid_Back_Off(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02065723159248861"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.S('si',('**','**'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../pkls/latin_lang_model.pk', 'wb') as fout:\n",
    "    pkl.dump(sb,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = pkl.dumps(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
