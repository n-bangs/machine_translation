{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still need to get sentences from books 1,2,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_list(soup):\n",
    "    l = []\n",
    "    for n in soup:\n",
    "        try:\n",
    "            l.append(int(n['n']))\n",
    "        except:\n",
    "            l.append(-1)  \n",
    "    return remove_duplicates(l)\n",
    "\n",
    "\n",
    "def remove_duplicates(l):\n",
    "    i = 0\n",
    "    while i+1 < len(l):\n",
    "        if l[i] == l[i+1]:\n",
    "            l[i+1] += 1\n",
    "        if l[i] > l[i+1] and l[i+1] != -1:\n",
    "            l[i] = l[i+1]-1\n",
    "        i += 1\n",
    "    return l\n",
    "\n",
    "def match(large,small):\n",
    "    l = make_list(large)\n",
    "    s = make_list(small)\n",
    "    li = 0\n",
    "    si = 0\n",
    "    groups = []\n",
    "    #print(l,s)\n",
    "    while si < len(s) and li < len(l):\n",
    "        if s[si] == -1:\n",
    "            si += 1\n",
    "            continue\n",
    "        if l[li] == -1:\n",
    "            li += 1\n",
    "            continue\n",
    "        lgroup = []\n",
    "        sgroup = []\n",
    "        if l[li] == s[si]:\n",
    "            lgroup.append(li)\n",
    "            sgroup.append(si)\n",
    "              \n",
    "        if si+1 < len(s) and li+1 < len(l): \n",
    "            if s[si+1] == -1:\n",
    "                si += 1\n",
    "            \n",
    "            if l[li+1] == -1:\n",
    "                li += 1\n",
    "            \n",
    "            \n",
    "            if l[li+1] != s[si+1]:\n",
    "                while l[li+1] != s[si+1]:\n",
    "                    if si+1 < len(s) and li+1 < len(l):\n",
    "                        if l[li+1] > s[si+1]:\n",
    "                            sgroup.append(si+1)\n",
    "                            si += 1\n",
    "                            if s[si] == -1:\n",
    "                                if si+1 < len(s):\n",
    "                                    si += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                        else:\n",
    "                            lgroup.append(li+1)\n",
    "                            li += 1\n",
    "                            if l[li] == -1:\n",
    "                                if li+1 < len(l):\n",
    "                                    li += 1\n",
    "                                else:\n",
    "                                    pass\n",
    "                    else:\n",
    "                        break\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup])\n",
    "            else:\n",
    "                si += 1\n",
    "                li += 1\n",
    "                groups.append([sgroup,lgroup]) \n",
    "        else:\n",
    "            break        \n",
    "    \n",
    "    groups.append([sgroup,lgroup])\n",
    "    return groups\n",
    "        \n",
    "\n",
    "def align_soups(m,small,large):\n",
    "    s_alignments = []\n",
    "    l_alignments = []\n",
    "    for g in m:\n",
    "        s = []\n",
    "        l = []\n",
    "        for i in g[0]:\n",
    "            s.append(small[i])\n",
    "        for i in g[1]:\n",
    "            l.append(large[i])\n",
    "        s_alignments.append(s)\n",
    "        l_alignments.append(l)   \n",
    "    return s_alignments,l_alignments\n",
    "\n",
    "\n",
    "def get_pairs(latin_soup, eng_soup, tag, typename, typos):\n",
    "    \"\"\" Given two soups, returns soups of equal size, with each element corresponding to the same\n",
    "        element in the other soup.\n",
    "        Lists may contain further lists of soups which are aligned with one another, they are combined into one list\n",
    "        in the first step of this function.\n",
    "    \"\"\"\n",
    "    lat_sect,eng_sect = [], []\n",
    "    \n",
    "    for i in range(len(latin_soup)):\n",
    "        lat_sect += latin_soup[i].findAll(tag, {typename: typos})\n",
    "    for i in range(len(eng_soup)):\n",
    "        eng_sect += eng_soup[i].findAll(tag, {typename: typos})\n",
    "\n",
    "    if len(lat_sect) > len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].parent['n'])\n",
    "            m = match(lat_sect,eng_sect)\n",
    "            eng_alignments,lat_alignments = align_soups(m,eng_sect,lat_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    elif len(lat_sect) < len(eng_sect):\n",
    "        try:\n",
    "            #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "            m = match(eng_sect,lat_sect)\n",
    "            lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "            return lat_alignments,eng_alignments\n",
    "        except:\n",
    "            print(\"error with {}, {}\".format(tag,typos))\n",
    "    else:\n",
    "        if eng_sect[0]['n'] == 'intro':\n",
    "            try:\n",
    "                #print(latin_soup[0]['n'],latin_soup[0].name)\n",
    "                m = match(eng_sect,lat_sect)\n",
    "                lat_alignments,eng_alignments = align_soups(m,lat_sect,eng_sect)\n",
    "                return lat_alignments,eng_alignments\n",
    "            except:\n",
    "                print(\"error with {}, {}\".format(tag,typos))\n",
    "        else:\n",
    "            \n",
    "            return lat_sect, eng_sect\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def align_pars(lsentences,esentences):\n",
    "    \n",
    "    if lsentences[-1] == '':\n",
    "        lsentences = lsentences[:-1]\n",
    "    if esentences[-1] == '':\n",
    "        esentences = esentences[:-1]\n",
    "        \n",
    "    if len(lsentences) > len(esentences):\n",
    "        alignments = align(lsentences,esentences)\n",
    "        target = esentences\n",
    "        source = lsentences\n",
    "    elif len(lsentences) < len(esentences):\n",
    "        alignments = align(esentences,lsentences)\n",
    "        target = lsentences\n",
    "        source = esentences\n",
    "    else:\n",
    "        return zip(lsentences,esentences)\n",
    "    \n",
    "    sentence_tuples = []\n",
    "    for k in alignments.keys():\n",
    "        sentence_tuples.append((target[k], [source[j] for j in alignments[k]]))\n",
    "        \n",
    "    return sentence_tuples\n",
    "          \n",
    "\n",
    "def get_text(soup):\n",
    "    \"\"\"Expects a soup in the form of a find('p') selection\"\"\"\n",
    "    s = []\n",
    "    for c in soup.children:\n",
    "        if c.name in [None,'corr', 'name']:\n",
    "            try:\n",
    "                s.append(c.text.strip())\n",
    "            except:\n",
    "                s.append(c.strip())\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_lemma(word):\n",
    "    try:\n",
    "        word = re.sub('[0-9]+', '', word)\n",
    "        word = re.sub('[-_]+', '', word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_lemmata_defs(latin):\n",
    "    words = lemmatizer.lemmatize(latin)\n",
    "    ldefs = []\n",
    "    unmatched = []\n",
    "\n",
    "    for word in words:\n",
    "        word = clean_lemma(word)\n",
    "        try:\n",
    "            ldefs.append(defs[word]['definition'].split(','))\n",
    "        except:\n",
    "            unmatched.append(word)\n",
    "    return ldefs,unmatched\n",
    "\n",
    "\n",
    "def get_defs(ldefs):\n",
    "    lldefs=[]\n",
    "    for l in ldefs:\n",
    "        lldefs += t(l[0])\n",
    "    return lldefs\n",
    "\n",
    "\n",
    "def get_match_count(english,latin):\n",
    "    \n",
    "    ldefs,unmatched = get_lemmata_defs(latin)\n",
    "    ldefs = get_defs(ldefs)\n",
    "    count = 0\n",
    "    for word in t(english):\n",
    "        if word in string.punctuation:\n",
    "            continue\n",
    "        if word in ldefs and word not in stop:\n",
    "            count += 1\n",
    "        if word in unmatched:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def align(source,target):\n",
    "    \"\"\" Takes a paragraph of sentences, source, and maps each sentence to a sentence in the \n",
    "        target paragraph. \n",
    "        \n",
    "        Returns a dictionary with the target sentence indices as keys, and the sentences in the source paragraph\n",
    "        which correspond to them as values.\n",
    "    \"\"\"\n",
    "    tsize = len(target)\n",
    "    ssize = len(source)\n",
    "    alignments = defaultdict(list)\n",
    "    alignments[0] = [0]\n",
    "    i = 1\n",
    "    j = 1\n",
    "    while tsize < ssize and i < len(source) and j < tsize:\n",
    "        max_count = get_match_count(source[i],target[j])\n",
    "        max_i = j\n",
    "        if j >= tsize-1:\n",
    "            rge = [j-1]\n",
    "        else:\n",
    "            rge = [j-1,j+1]\n",
    "        for r in rge: \n",
    "            c = get_match_count(source[i],target[r])\n",
    "            if c > max_count:\n",
    "                max_count = c\n",
    "                max_i = r\n",
    "\n",
    "        if max_i != j:\n",
    "            ssize -= 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        alignments[max_i].append(i)\n",
    "        i += 1\n",
    "        \n",
    "  \n",
    "    while j < tsize:\n",
    "        alignments[j].append(i)\n",
    "        j += 1\n",
    "        i += 1\n",
    "    alignments[j-1].extend([k for k in range(i,len(source))])\n",
    "    \n",
    "    return alignments\n",
    "\n",
    "def get_sec_alignments(lpar,epar):\n",
    "    if len(lpar) > len(epar):\n",
    "        if len(epar) > 1:\n",
    "            alignments = align(lpar,epar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(lpar[v])\n",
    "                aligned.append((group,epar[k]))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [('. '.join(lpar),epar[0])] \n",
    "            except:\n",
    "                print('error, {}'.format(epar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    elif len(epar) > len(lpar):\n",
    "        if len(lpar) > 1:\n",
    "            alignments = align(epar,lpar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(epar[v])\n",
    "                aligned.append((lpar[k],group))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [(lpar[0],'. '.join(epar))]\n",
    "            except:\n",
    "                print('error, {}'.format(lpar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    else:\n",
    "        aligned = list(zip(lpar,epar))\n",
    "        \n",
    "    return aligned\n",
    "\n",
    "def get_sentences(chap_soup, sect_soup):\n",
    "    t = bs4.element.Tag\n",
    "    sect = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    c = 0\n",
    "    children = list(chap_soup.children)\n",
    "    soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "    c += 1\n",
    "    while type(soup) != t:\n",
    "        soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "        c += 1\n",
    "    i += 1\n",
    "    while c < len(children):\n",
    "        print(sentences)\n",
    "        while soup.next_sibling != None:\n",
    "            soup = soup.next_sibling\n",
    "            if soup.name == 'milestone':\n",
    "                try:\n",
    "                    if int(soup['n']) == int(sect_soup[i]['n']):\n",
    "                        i += 1\n",
    "                        sentences.append(' '.join(sect))\n",
    "                        sect = []\n",
    "                except:\n",
    "                    soup = soup.next_sibling\n",
    "                    i += 1\n",
    "            if soup.name in ['corr', 'name','hi','quote','reg', 'l', 'lg', 'placeName']:\n",
    "                    sect.append(soup.text.strip())\n",
    "            elif soup.name == None:\n",
    "                    sect.append(soup.strip())\n",
    "                    \n",
    "        sentences.append(' '.join(sect))\n",
    "        sect = []\n",
    "        try:\n",
    "            soup = children[c].find('milestone')#, {'n':sect_soup[i]['n']})\n",
    "            c += 1\n",
    "            while type(soup) != t and c < len(children):\n",
    "                soup = children[c].find('milestone')\n",
    "                c += 1\n",
    "            i += 1\n",
    "        except:\n",
    "            continue\n",
    "    sentences.append(' '.join(sect))\n",
    "    sect = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = wordpunct_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "f = open('lemmas.json')\n",
    "defs = json.loads(f.read())\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "tokenizer = WordTokenizer('latin')\n",
    "s_tokenizer = TokenizeSentence('latin')\n",
    "\n",
    "f = open('../Latin/Caesar/opensource/caes.bc_lat.xml')\n",
    "latin_soup = BeautifulSoup(f.read(),'lxml')\n",
    "f.close()\n",
    "\n",
    "f = open('../Latin/Caesar/opensource/caes.bc_eng.xml')\n",
    "eng_soup = BeautifulSoup(f.read(),'lxml')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with 0\n",
      "b:  0\n",
      "b:  1\n",
      "b:  2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-019e3f9d1b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlchaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mechaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlchaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lat,eng = get_pairs([latin_soup],[eng_soup],'div1','type','book')\n",
    "\n",
    "lchaps = []\n",
    "echaps = []\n",
    "for i in range(len(lat)):\n",
    "    try:\n",
    "        p = get_pairs([lat[i]],[eng[i]],'div2','type','chapter')\n",
    "        if type(p[0][0]) == list:\n",
    "            lchaps.append([s[0] for s in p[0]])\n",
    "            echaps.append([s[0] for s in p[1]])\n",
    "        else:\n",
    "            lchaps.append(p[0])\n",
    "            echaps.append(p[1])\n",
    "    except:\n",
    "        print('error with {}'.format(i))\n",
    "\n",
    "alignments = []\n",
    "for b in range(len(lat)):\n",
    "    print('b: ',b)\n",
    "    for c in range(len(lchaps[b])):\n",
    "        es = ' '.join(echaps[b][c].text.replace('\\n',' ').split())\n",
    "        ls = lchaps[b][c].text.replace('\\n', ' ')\n",
    "\n",
    "        ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "        es = re.sub(r'\\. \\. \\.','',es)\n",
    "        ls = re.sub('[:;,]','',ls)\n",
    "        es = re.sub('[:;,]','',es)\n",
    "\n",
    "        epar = sent_tokenize(es)\n",
    "        lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "        alignments.append(get_sec_alignments(lpar,epar))\n",
    "        #sec_alignments.append(get_sec_alignments(lpar[p],epar[p]))\n",
    "        #alignments.append(sec_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = get_pairs([lat[0]],[eng[0]],'div2','type','chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignoring first chapter of first book, some sort of reading error occured. \n",
    "\n",
    "lch_1 = p[0][1:]\n",
    "ech_1 = p[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lch_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es = ' '.join(ech_1[0][0].text.replace('\\n',' ').split())\n",
    "ls = lch_1[0][0].text.replace('\\n', ' ')\n",
    "\n",
    "ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "es = re.sub(r'\\. \\. \\.','',es)\n",
    "ls = re.sub('[:;,]','',ls)\n",
    "es = re.sub('[:;,]','',es)\n",
    "\n",
    "epar = sent_tokenize(es)\n",
    "lpar = s_tokenizer.tokenize_sentences(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(lch_1)):\n",
    "    es = ' '.join(ech_1[c][0].text.replace('\\n',' ').split())\n",
    "    ls = lch_1[c][0].text.replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "\n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "    for s in a:\n",
    "        count += len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('  Id vero militibus fuit pergratum et iucundum ut  ex ipsa significatione cognosci potuit ut qui aliquid iusti incommodi expectavissent ultro praemium missionis ferrent.',\n",
       " [\"These conditions were agreeable to Afranius's soldiers who instead of being punished as they feared were in some sort rewarded by the discharge procured them.\"])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[-2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2566"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['  Commisso proelio Massiliensibus res nulla ad  virtutem defuit.'],\n",
       "  'Accordingly in the engagement they behaved with the most determined courage.'),\n",
       " (['sed memores eorum praeceptorum quae paulo ante ab suis acceperant hoc animo decertabant ut nullum aliud tempus ad conandum habituri viderentur et quibus in pugna vitae periculum accideret non ita multo se reliquorum civium fatum antecedere existimarent quibus urbe capta eadem esset   belli fortuna patienda.'],\n",
       "  'The remembrance of what their wives and children had represented to them at their departure served to exalt their bravery in a full persuasion that this was the last opportunity they should have of exerting themselves in defence of their country and that if they fell in the engagement their fellow-citizens could not long survive them as their fate must be the same upon the taking of the town.'),\n",
       " (['diductisque nostris paulatim navibus et artificio gubernatorum et mobilitati navium locus dabatur et siquando nostri facultatem nacti ferreis manibus iniectis navem religaverant undique suis   laborantibus succurrebant.'],\n",
       "  \"Our ships being at some distance from each other both gave the enemy's pilots an opportunity of showing their address in working their vessels and flying to the assistance of their friends when they were laid hold on by our grappling hooks.\"),\n",
       " (['neque vero coniuncti Albici comminus pugnando deficiebant neque multum cedebant virtute nostris.'],\n",
       "  'And indeed when it came to a close fight they seconded the mountaineers with wonderful resolution and in bravery seemed to yield but little to our men.'),\n",
       " (['simul ex minoribus navibus magna vis eminus missa telorum multa nostris de improviso imprudentibus atque inpeditis vulnera inferebant.'],\n",
       "  'At the same time a great quantity of darts poured incessantly from their smaller frigates wounded a great many of our rowers and such of the soldiers as were without shelter.'),\n",
       " (['conspicataeque naves triremes duae navem D. Bruti quae ex insigni facile agnosci poterat duabus  ex partibus sese in eam incitaverant.',\n",
       "   'sed tantum re provisa Brutus celeritate navis enisus est ut parvo   momento antecederet.',\n",
       "   'illae adeo graviter inter se incitatae conflixerunt ut vehementissime utraque ex concursu laborarent altera vero praefracto rostro tota   conlabefieret.',\n",
       "   'qua re animadversa quae proximae ei loco ex Bruti classe naves erant in eas inpeditas impetum faciunt celeriterque ambas deprimunt. '],\n",
       "  'Two of their galleys fell upon that of Brutus which was easily distinguished by its flag but though they attacked him on both sides he extricated himself with such agility and address as in a short time to get a little before which made them run foul of each other so violently that they were both considerably shattered one in particular had its beak broken and was in a manner totally crushed which being observed by those of our fleet that lay nearest they suddenly fell upon and sunk them before they could recover out of their disorder.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('aligned_sentences/caes_bc_sentences.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../Latin/Caesar/opensource/caes.bg_lat.xml')\n",
    "latin_soup = BeautifulSoup(f.read(),'lxml')\n",
    "f.close()\n",
    "\n",
    "f = open('../Latin/Caesar/opensource/caes.bg_eng.xml')\n",
    "eng_soup = BeautifulSoup(f.read(),'lxml')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_soup.findAll('div1',{'type':'Book'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat,eng = get_pairs([latin_soup],[eng_soup],'div1','type','Book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0,3,4,5,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng[2].findAll('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lat[2].findAll('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpars = []\n",
    "for p in lat[0].findAll('p'):\n",
    "    ls = p.text.replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    ls = re.sub('[:;,]','',ls)\n",
    "    lpars.append(len(s_tokenizer.tokenize_sentences(ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pars = []\n",
    "for p in eng[0].findAll('p'):\n",
    "    es = ' '.join(p.text.replace('\\n',' ').split())\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    es = re.sub('[:;,]','',es)\n",
    "    pars.append(len(sent_tokenize(es)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l = list(zip(pars,lpars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (7, 8)\n",
      "1 (6, 5)\n",
      "2 (6, 7)\n",
      "3 (3, 4)\n",
      "4 (4, 3)\n",
      "5 (8, 7)\n",
      "6 (5, 7)\n",
      "7 (4, 4)\n",
      "8 (4, 4)\n",
      "9 (8, 6)\n",
      "10 (4, 5)\n",
      "11 (8, 8)\n",
      "12 (5, 6)\n",
      "13 (6, 7)\n",
      "14 (5, 6)\n",
      "15 (3, 4)\n",
      "16 (2, 4)\n",
      "17 (5, 9)\n",
      "18 (4, 4)\n",
      "19 (5, 6)\n",
      "20 (5, 5)\n",
      "21 (5, 5)\n",
      "22 (3, 3)\n",
      "23 (3, 3)\n",
      "24 (7, 7)\n",
      "25 (10, 9)\n",
      "26 (4, 4)\n",
      "27 (4, 4)\n",
      "28 (4, 4)\n",
      "29 (3, 3)\n",
      "30 (15, 16)\n",
      "31 (5, 5)\n",
      "32 (5, 5)\n",
      "33 (3, 4)\n",
      "34 (5, 2)\n",
      "35 (4, 8)\n",
      "36 (3, 3)\n",
      "37 (5, 4)\n",
      "38 (8, 7)\n",
      "39 (18, 17)\n",
      "40 (4, 4)\n",
      "41 (6, 7)\n",
      "42 (9, 10)\n",
      "43 (12, 17)\n",
      "44 (2, 3)\n",
      "45 (4, 4)\n",
      "46 (8, 8)\n",
      "47 (7, 5)\n",
      "48 (6, 6)\n",
      "49 (6, 6)\n",
      "50 (4, 3)\n",
      "51 (8, 7)\n",
      "52 (10, 8)\n",
      "53 (3, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(l)):\n",
    "    print(i,l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Having drawn out all his forces at the break of day, and marshaled them in a double line, he posted the auxiliaries in the center, and waited to see what measures the enemy would take. They, although on account of their great number and their ancient renown in war, and the small number of our men, they supposed they might safely fight, nevertheless considered it safer to gain the victory without any wound, by besetting the passes [and] cutting off the provisions: and if the Romans, on account of the want of corn, should begin to retreat, they intended to attack them while encumbered in their march and depressed in spirit [as being assailed while] under baggage. This measure being approved of by the leaders and the forces of the Romans drawn out, the enemy [still] kept themselves in their camp. Crassus having remarked this circumstance, since the enemy, intimidated by their own delay, and by the reputation [i.e. for cowardice arising thence] had rendered our soldiers more eager for fighting, and the remarks of all were heard [declaring] that no longer ought delay to be made in going to the camp, after encouraging his men, he marches to the camp of the enemy, to the great gratification of his own troops.)'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(eng[2].findAll('p')[23].text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crassus equitum praefectos cohortatus, ut magnis praemiis pollicitationibusque suos excitarent, quid fieri vellet ostendit. Illi, ut erat imperatum, eductis iis cohortibus quae praesidio castris relictae intritae ab labore erant, et longiore itinere circumductis, ne ex hostium castris conspici possent, omnium oculis mentibusque ad pugnam intentis celeriter ad eas quas diximus munitiones pervenerunt atque his prorutis prius in hostium castris constiterunt quam plane ab his videri aut quid rei gereretur cognosci posset. Tum vero clamore ab ea parte audito nostri redintegratis viribus, quod plerumque in spe victoriae accidere consuevit, acrius impugnare coeperunt. Hostes undique circumventi desperatis omnibus rebus se per munitiones deicere et fuga salutem petere contenderunt. Quos equitatus apertissimis campis consectatus ex milium L numero, quae ex Aquitania Cantabrisque convenisse constabat, vix quarta parte relicta, multa nocte se in castra recepit.'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat[2].findAll('p')[23].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = ' '.join(ech_1[c][0].text.replace('\\n',' ').split())\n",
    "ls = lch_1[c][0].text.replace('\\n', ' ')\n",
    "\n",
    "ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "es = re.sub(r'\\. \\. \\.','',es)\n",
    "ls = re.sub('[:;,]','',ls)\n",
    "es = re.sub('[:;,]','',es)\n",
    "\n",
    "epar = sent_tokenize(es)\n",
    "lpar = s_tokenizer.tokenize_sentences(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>When <name type=\"pers\">Caesar</name> was setting out for <name key=\"tgn,1000080\" reg=\"Italy [12.833,42.833] (nation), Europe\" type=\"place\">Italy</name>, he\n",
       "                    sent <name>Servius Galba</name> with the twelfth legion and part of the cavalry,\n",
       "                    against the <name>Nantuates</name>, the <name>Veragri</name>, and\n",
       "                        <name>Seduni</name>, who extend from the territories of the\n",
       "                        <name>Allobroges</name>, and the lake of <name key=\"tgn,7007279\" reg=\"  +Geneve [6.15,46.216] (inhabited place), Geneve, Switzerland, Europe \" type=\"place\">Geneva </name>, and the River <name key=\"tgn,7023890\" reg=\"Rhone [4.833,43.333] (river), Europe\" type=\"place\">Rhone </name> to the top of the\n",
       "                        <name key=\"tgn,7007746\" reg=\"Alps (mountain system), Europe\" type=\"place\">Alps</name>. The reason for sending him was, that he desired that the pass\n",
       "                    along the <name key=\"tgn,7007746\" reg=\"Alps (mountain system), Europe\" type=\"place\">Alps </name>, through which [the\n",
       "                        <name>Roman]</name> merchants had been accustomed to travel with great\n",
       "                    danger, and under great imposts, should be opened. He permitted him, if he\n",
       "                    thought it necessary, to station the legion in these places, for the purpose of\n",
       "                    wintering. <name>Galba</name> having fought some successful battles and stormed\n",
       "                    several of their forts, upon embassadors being sent to him from all parts and\n",
       "                    hostages given and a peace concluded, determined to station two cohorts among\n",
       "                    the <name>Nantuates</name>, and to winter in person with the other cohorts of\n",
       "                    that legion in a village of the <name>Veragri</name>, which is called\n",
       "                        <name>Octodurus</name>; and this village being situated in a valley, with a\n",
       "                    small plain annexed to it, is bounded on all sides by very high mountains. As\n",
       "                    this village was divided into two parts by a river, he granted one part of it to\n",
       "                    the <name type=\"ethnic\">Gauls</name>, and assigned the other, which had been\n",
       "                    left by them unoccupied, to the cohorts to winter in. He fortified this [latter]\n",
       "                    part with a rampart and a ditch. <milestone n=\"2\" unit=\"chapter\"></milestone></p>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng[2].findAll('p')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p><milestone n=\"1\" unit=\"chapter\"></milestone><milestone n=\"1\" unit=\"section\"></milestone>Cum in Italiam proficisceretur Caesar, Ser. Galbam cum legione XII. et parte equitatus in Nantuates, Veragros Sedunosque misit, qui a finibus Allobrogum et lacu Lemanno et flumine Rhodano ad summas Alpes pertinent. <milestone n=\"2\" unit=\"section\"></milestone>Causa mittendi fuit quod iter per Alpes, quo magno cum periculo magnisque cum portoriis mercatores ire consuerant, patefieri volebat. <milestone n=\"3\" unit=\"section\"></milestone>Huic permisit, si opus esse arbitraretur, uti in his locis legionem hiemandi causa conlocaret. <milestone n=\"4\" unit=\"section\"></milestone>Galba secundis aliquot proeliis factis castellisque compluribus eorum expugnatis, missis ad eum undique legatis obsidibusque datis et pace facta, constituit cohortes duas in Nantuatibus conlocare et ipse cum reliquis eius legionis cohortibus in vico Veragrorum, qui appellatur Octodurus hiemare; <milestone n=\"5\" unit=\"section\"></milestone>qui vicus positus in valle non magna adiecta planitie altissimis montibus undique continetur. <milestone n=\"6\" unit=\"section\"></milestone>Cum hic in duas partes flumine divideretur, alteram partem eius vici Gallis [ad hiemandum] concessit, alteram vacuam ab his relictam cohortibus attribuit. Eum locum vallo fossaque munivit.</p>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat[2].findAll('p')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with 0\n",
      "error with 1\n",
      "error with 2\n",
      "error with 3\n",
      "error with 4\n",
      "error with 5\n",
      "error with 6\n",
      "error with 7\n"
     ]
    }
   ],
   "source": [
    "lchaps = []\n",
    "echaps = []\n",
    "for i in range(len(lat)):\n",
    "    try:\n",
    "        p = get_pairs([lat[i]],[eng[i]],'div2','type','chapter')\n",
    "        if type(p[0][0]) == list:\n",
    "            lchaps.append([s[0] for s in p[0]])\n",
    "            echaps.append([s[0] for s in p[1]])\n",
    "        else:\n",
    "            lchaps.append(p[0])\n",
    "            echaps.append(p[1])\n",
    "    except:\n",
    "        print('error with {}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  0\n",
      "b:  3\n",
      "b:  4\n",
      "b:  5\n",
      "b:  7\n"
     ]
    }
   ],
   "source": [
    "l = [0,3,4,5,7]\n",
    "alignments = []\n",
    "for b in l:\n",
    "    print('b: ',b)\n",
    "    lp = lat[b].findAll('p')\n",
    "    ep = eng[b].findAll('p')\n",
    "    for i in range(len(lp)):\n",
    "    #for c in range(len(lat[b])):\n",
    "        es = ' '.join(ep[i].text.replace('\\n',' ').split())\n",
    "        ls = lp[i].text.replace('\\n', ' ')\n",
    "\n",
    "        ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "        es = re.sub(r'\\. \\. \\.','',es)\n",
    "        ls = re.sub('[:;,]','',ls)\n",
    "        es = re.sub('[:;,]','',es)\n",
    "\n",
    "        epar = sent_tokenize(es)\n",
    "        lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "        alignments.append(get_sec_alignments(lpar,epar))\n",
    "        #sec_alignments.append(get_sec_alignments(lpar[p],epar[p]))\n",
    "        #alignments.append(sec_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "    for s in a:\n",
    "        count += len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2320"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hac oratione habita mirum in modum conversae sunt omnium mentes summaque alacritas et cupiditas belli gerendi innata est princepsque X. legio per tribunos militum ei gratias egit quod de se optimum iudicium fecisset seque esse ad bellum gerendum paratissimam confirmavit.',\n",
       "  'Upon the delivery of this speech the minds of all were changed in a surprising manner and the highest ardor and eagerness for prosecuting the war were engendered and the tenth legion was the first to return thanks to him through their military tribunes for his having expressed this most favorable opinion of them and assured him that they were quite ready to prosecute the war.'),\n",
       " ('Deinde reliquae legiones cum tribunis militum et primorum ordinum centurionibus egerunt uti Caesari satis facerent se neque umquam dubitasse neque timuisse neque de summa belli suum iudicium sed imperatoris esse existimavisse.',\n",
       "  \"Then the other legions endeavored through their military tribunes and the centurions of the principal companies to excuse themselves to Caesar [saying] that they had never either doubted or feared or supposed that the determination of the conduct of the war was theirs and not their general's.\"),\n",
       " ('Eorum satisfactione accepta et itinere exquisito per Diviciacum quod ex Gallis ei maximam fidem habebat ut milium amplius quinquaginta circuitu locis apertis exercitum duceret de quarta vigilia ut dixerat profectus est.',\n",
       "  'Having accepted their excuse and having had the road carefully reconnoitered by Divitiacus because in him of all others he had the greatest faith [he found] that by a circuitous route of more than fifty miles he might lead his army through open parts he then set out in the fourth watch as he had said [he would].'),\n",
       " ('Septimo die cum iter non intermitteret ab exploratoribus certior factus est Ariovisti copias a nostris milia passuum IIII et XX abesse.',\n",
       "  'On the seventh day as he did not discontinue his march he was informed by scouts that the forces of Ariovistus were only four and twenty miles distant from ours.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('aligned_sentences/caes_bg_sentences_03457.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
