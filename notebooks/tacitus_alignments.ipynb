{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def align_pars(lsentences,esentences):\n",
    "    \n",
    "    if lsentences[-1] == '':\n",
    "        lsentences = lsentences[:-1]\n",
    "    if esentences[-1] == '':\n",
    "        esentences = esentences[:-1]\n",
    "        \n",
    "    if len(lsentences) > len(esentences):\n",
    "        alignments = align(lsentences,esentences)\n",
    "        target = esentences\n",
    "        source = lsentences\n",
    "    elif len(lsentences) < len(esentences):\n",
    "        alignments = align(esentences,lsentences)\n",
    "        target = lsentences\n",
    "        source = esentences\n",
    "    else:\n",
    "        return zip(lsentences,esentences)\n",
    "    \n",
    "    sentence_tuples = []\n",
    "    for k in alignments.keys():\n",
    "        sentence_tuples.append((target[k], [source[j] for j in alignments[k]]))\n",
    "        \n",
    "    return sentence_tuples\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "def clean_lemma(word):\n",
    "    try:\n",
    "        word = re.sub('[0-9]+', '', word)\n",
    "        word = re.sub('[-_]+', '', word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_lemmata_defs(latin):\n",
    "    words = lemmatizer.lemmatize(latin)\n",
    "    ldefs = []\n",
    "    unmatched = []\n",
    "\n",
    "    for word in words:\n",
    "        word = clean_lemma(word)\n",
    "        try:\n",
    "            ldefs.append(defs[word]['definition'].split(','))\n",
    "        except:\n",
    "            unmatched.append(word)\n",
    "    return ldefs,unmatched\n",
    "\n",
    "\n",
    "def get_defs(ldefs):\n",
    "    lldefs=[]\n",
    "    for l in ldefs:\n",
    "        lldefs += t(l[0])\n",
    "    return lldefs\n",
    "\n",
    "\n",
    "def get_match_count(english,latin):\n",
    "    \n",
    "    ldefs,unmatched = get_lemmata_defs(latin)\n",
    "    ldefs = get_defs(ldefs)\n",
    "    count = 0\n",
    "    for word in t(english):\n",
    "        if word in string.punctuation:\n",
    "            continue\n",
    "        if word in ldefs and word not in stop:\n",
    "            count += 1\n",
    "        if word in unmatched:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def align(source,target):\n",
    "    \"\"\" Takes a paragraph of sentences, source, and maps each sentence to a sentence in the \n",
    "        target paragraph. \n",
    "        \n",
    "        Returns a dictionary with the target sentence indices as keys, and the sentences in the source paragraph\n",
    "        which correspond to them as values.\n",
    "    \"\"\"\n",
    "    tsize = len(target)\n",
    "    ssize = len(source)\n",
    "    alignments = defaultdict(list)\n",
    "    alignments[0] = [0]\n",
    "    i = 1\n",
    "    j = 1\n",
    "    while tsize < ssize and i < len(source) and j < tsize:\n",
    "        max_count = get_match_count(source[i],target[j])\n",
    "        max_i = j\n",
    "        if j >= tsize-1:\n",
    "            rge = [j-1]\n",
    "        else:\n",
    "            rge = [j-1,j+1]\n",
    "        for r in rge: \n",
    "            c = get_match_count(source[i],target[r])\n",
    "            if c > max_count:\n",
    "                max_count = c\n",
    "                max_i = r\n",
    "\n",
    "        if max_i != j:\n",
    "            ssize -= 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        alignments[max_i].append(i)\n",
    "        i += 1\n",
    "        \n",
    "  \n",
    "    while j < tsize:\n",
    "        alignments[j].append(i)\n",
    "        j += 1\n",
    "        i += 1\n",
    "    alignments[j-1].extend([k for k in range(i,len(source))])\n",
    "    \n",
    "    return alignments\n",
    "\n",
    "def get_sec_alignments(lpar,epar):\n",
    "    if len(lpar) > len(epar):\n",
    "        if len(epar) > 1:\n",
    "            alignments = align(lpar,epar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(lpar[v])\n",
    "                aligned.append((group,epar[k]))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [('. '.join(lpar),epar[0])] \n",
    "            except:\n",
    "                print('error, {}'.format(epar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    elif len(epar) > len(lpar):\n",
    "        if len(lpar) > 1:\n",
    "            alignments = align(epar,lpar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(epar[v])\n",
    "                aligned.append((lpar[k],group))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [(lpar[0],'. '.join(epar))]\n",
    "            except:\n",
    "                print('error, {}'.format(lpar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    else:\n",
    "        aligned = list(zip(lpar,epar))\n",
    "        \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = wordpunct_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "f = open('../lemmas.json')\n",
    "defs = json.loads(f.read())\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "tokenizer = WordTokenizer('latin')\n",
    "s_tokenizer = TokenizeSentence('latin')\n",
    "\n",
    "f = open('../pkls/tacitus_paragraphs.json')\n",
    "paragraphs = json.loads(f.read())\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "lat_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for p in paragraphs:\n",
    "    eng_text.append(p[1])\n",
    "    lat_text.append(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alignments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in range(len(eng_text)):\n",
    "    es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "    ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "    ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "    es = re.sub(r'\\. \\. \\.','',es)\n",
    "    ls = re.sub(r'[:;,]','',ls)\n",
    "    es = re.sub(r'[:;,]','',es)\n",
    "    ls = re.sub(r'(\\d)+\\.','',ls)\n",
    "    es = re.sub(r'(\\d)+\\.','',es)\n",
    "\n",
    "    \n",
    "    epar = sent_tokenize(es)\n",
    "    lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "    alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "    count += len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  At Caesar cognita morte legati ne provincia sine rectore foret A. Didium suffecit.',\n",
       "  [' The emperor on hearing of the death of his representative appointed Aulus Didius in his place that the province might not be left without a governor.']),\n",
       " ('is  propere vectus non tamen integras res invenit adversa interim legionis pugna cui Manlius Valens  praeerat auctaque et apud hostis eius rei fama quo venientem ducem exterrerrent atque illo augente  audita ut maior laus compositis et si duravissent venia iustior tribueretur.',\n",
       "  ['Didius though he quickly arrived found matters far from prosperous for the legion under the command of Manlius Valens had meanwhile been defeated and the disaster had been exaggerated by the enemy to alarm the new general while he again magnified it that he might win the more glory by quelling the movement or have a fairer excuse if it lasted.']),\n",
       " ('Silures id quoque damnum  intulerant lateque persultabant donec adcursu Didii pellerentur.',\n",
       "  ['This loss too had been inflicted on us by the Silures and they were scouring the country far and wide till Didius hurried up and dispersed them.']),\n",
       " ('sed post captum Caratacum  praecipuus scientia rei militaris Venutius e Brigantum civitate ut supra memoravi fidusque diu et  Romanis armis defensus cum Cartimanduam reginam matrimonio teneret mox orto discidio et statim  bello etiam adversus nos hostilia induerat sed primo tantum inter ipsos certabatur callidisque  Cartimandua artibus fratrem ac propinquos Venutii intercepit.',\n",
       "  ['After the capture of Caractacus Venutius of the Brigantes as I have already mentioned was pre-eminent in military skill he had long been loyal to Rome and had been defended by our arms while he was united in marriage to the queen Cartismandua.',\n",
       "   'Subsequently a quarrel broke out between them followed instantly by war and he then assumed a hostile attitude also towards us.',\n",
       "   'At first however they simply fought against each other and Cartismandua by cunning stratagems captured the brothers and kinsfolk of Venutius.']),\n",
       " ('inde accensi hostes stimulante  ignominia ne feminae imperio subderentur valida et lecta armis iuventus regnum eius invadunt.',\n",
       "  ['This enraged the enemy who were stung with shame at the prospect of falling under the dominion of a woman.']),\n",
       " ('quod  nobis praevisum et missae auxilio cohortes acre proelium fecere cuius initio ambiguo finis laetior  fuit.',\n",
       "  ['The flower of their youth picked out for war invaded her kingdom.',\n",
       "   'This we had foreseen some cohorts were sent to her aid and a sharp contest followed which was at first doubtful but had a satisfactory termination.']),\n",
       " ('neque dispari eventu pugnatum a legione cui Caesius Nasica praeerat nam Didius senectute  gravis et multa copia honorum per ministros agere et arcere hostem satis habebat.',\n",
       "  ['The legion under the command of Caesius Nasica fought with a similar result.']),\n",
       " ('haec quamquam  a duobus pro praetoribus pluris per annos gesta coniunxi ne divisa haud perinde ad memoriam sui  valerent ad temporum ordinem redeo.  ',\n",
       "  ['For Didius burdened with years and covered with honours was content with acting through his officers and merely holding back the enemy.',\n",
       "   'These transactions though occurring under two propraetors and occupying several years I have closely connected lest if related separately they might be less easily remembered.',\n",
       "   'I now return to the chronological order.'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
