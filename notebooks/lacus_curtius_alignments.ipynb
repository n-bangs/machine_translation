{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def align_pars(lsentences,esentences):\n",
    "    \n",
    "    if lsentences[-1] == '':\n",
    "        lsentences = lsentences[:-1]\n",
    "    if esentences[-1] == '':\n",
    "        esentences = esentences[:-1]\n",
    "        \n",
    "    if len(lsentences) > len(esentences):\n",
    "        alignments = align(lsentences,esentences)\n",
    "        target = esentences\n",
    "        source = lsentences\n",
    "    elif len(lsentences) < len(esentences):\n",
    "        alignments = align(esentences,lsentences)\n",
    "        target = lsentences\n",
    "        source = esentences\n",
    "    else:\n",
    "        return zip(lsentences,esentences)\n",
    "    \n",
    "    sentence_tuples = []\n",
    "    for k in alignments.keys():\n",
    "        sentence_tuples.append((target[k], [source[j] for j in alignments[k]]))\n",
    "        \n",
    "    return sentence_tuples\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "def clean_lemma(word):\n",
    "    try:\n",
    "        word = re.sub('[0-9]+', '', word)\n",
    "        word = re.sub('[-_]+', '', word)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_lemmata_defs(latin):\n",
    "    words = lemmatizer.lemmatize(latin)\n",
    "    ldefs = []\n",
    "    unmatched = []\n",
    "\n",
    "    for word in words:\n",
    "        word = clean_lemma(word)\n",
    "        try:\n",
    "            ldefs.append(defs[word]['definition'].split(','))\n",
    "        except:\n",
    "            unmatched.append(word)\n",
    "    return ldefs,unmatched\n",
    "\n",
    "\n",
    "def get_defs(ldefs):\n",
    "    lldefs=[]\n",
    "    for l in ldefs:\n",
    "        lldefs += t(l[0])\n",
    "    return lldefs\n",
    "\n",
    "\n",
    "def get_match_count(english,latin):\n",
    "    \n",
    "    ldefs,unmatched = get_lemmata_defs(latin)\n",
    "    ldefs = get_defs(ldefs)\n",
    "    count = 0\n",
    "    for word in t(english):\n",
    "        if word in string.punctuation:\n",
    "            continue\n",
    "        if word in ldefs and word not in stop:\n",
    "            count += 1\n",
    "        if word in unmatched:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def align(source,target):\n",
    "    \"\"\" Takes a paragraph of sentences, source, and maps each sentence to a sentence in the \n",
    "        target paragraph. \n",
    "        \n",
    "        Returns a dictionary with the target sentence indices as keys, and the sentences in the source paragraph\n",
    "        which correspond to them as values.\n",
    "    \"\"\"\n",
    "    tsize = len(target)\n",
    "    ssize = len(source)\n",
    "    alignments = defaultdict(list)\n",
    "    alignments[0] = [0]\n",
    "    i = 1\n",
    "    j = 1\n",
    "    while tsize < ssize and i < len(source) and j < tsize:\n",
    "        max_count = get_match_count(source[i],target[j])\n",
    "        max_i = j\n",
    "        if j >= tsize-1:\n",
    "            rge = [j-1]\n",
    "        else:\n",
    "            rge = [j-1,j+1]\n",
    "        for r in rge: \n",
    "            c = get_match_count(source[i],target[r])\n",
    "            if c > max_count:\n",
    "                max_count = c\n",
    "                max_i = r\n",
    "\n",
    "        if max_i != j:\n",
    "            ssize -= 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        alignments[max_i].append(i)\n",
    "        i += 1\n",
    "        \n",
    "  \n",
    "    while j < tsize:\n",
    "        alignments[j].append(i)\n",
    "        j += 1\n",
    "        i += 1\n",
    "    alignments[j-1].extend([k for k in range(i,len(source))])\n",
    "    \n",
    "    return alignments\n",
    "\n",
    "def get_sec_alignments(lpar,epar):\n",
    "    if len(lpar) > len(epar):\n",
    "        if len(epar) > 1:\n",
    "            alignments = align(lpar,epar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(lpar[v])\n",
    "                aligned.append((group,epar[k]))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [('. '.join(lpar),epar[0])] \n",
    "            except:\n",
    "                print('error, {}'.format(epar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    elif len(epar) > len(lpar):\n",
    "        if len(lpar) > 1:\n",
    "            alignments = align(epar,lpar)\n",
    "            aligned = []\n",
    "            for k in alignments.keys():\n",
    "                group = []\n",
    "                for v in alignments[k]:\n",
    "                    group.append(epar[v])\n",
    "                aligned.append((lpar[k],group))\n",
    "        else:\n",
    "            try:\n",
    "                aligned = [(lpar[0],'. '.join(epar))]\n",
    "            except:\n",
    "                print('error, {}'.format(lpar))\n",
    "                aligned = list(zip(lpar,epar))\n",
    "    else:\n",
    "        aligned = list(zip(lpar,epar))\n",
    "        \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = wordpunct_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "f = open('../lemmas.json')\n",
    "defs = json.loads(f.read())\n",
    "\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "tokenizer = WordTokenizer('latin')\n",
    "s_tokenizer = TokenizeSentence('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = '/Users/nickybangs/ds/metis/machine_translation/lacus_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "file_pairs = []\n",
    "while i < len(files)-1:\n",
    "    efname = dirname + files[i]\n",
    "    lfname = dirname + files[i+1]\n",
    "    file_pairs.append((efname,lfname))\n",
    "    i += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/Users/nickybangs/ds/metis/machine_translation/lacus_files/einsiedeln_eclogues_1_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/einsiedeln_eclogues_1_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/einsiedeln_eclogues_2_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/einsiedeln_eclogues_2_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1A_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1A_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1B_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1B_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1C_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1C_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1D_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1D_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1E_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1E_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1F_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1F_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1G_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1G_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1H_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1H_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1I_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1I_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1J_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1J_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1K_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1K_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1L_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1L_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1M_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_1M_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2A_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2A_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2B_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2B_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2C_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2C_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2D_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2D_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2E_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2E_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2F_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2F_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2G_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2G_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2H_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2H_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2I_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/florus_epitome_2I_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/frontinus_strategemata_1_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/frontinus_strategemata_1_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/frontinus_strategemata_2_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/frontinus_strategemata_2_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/frontinus_strategemata_4_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/frontinus_strategemata_4_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/grattius_cynegeticon_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/grattius_cynegeticon_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/laus_pisonis_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/laus_pisonis_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_cynegeticon_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_cynegeticon_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_de_Aucupio_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_de_Aucupio_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_2_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_2_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_3_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_3_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_4_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_4_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/nemesianus_eclogues_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_1_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_1_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2A_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2A_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2B_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2B_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2C_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2C_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2D_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/paterculus_historiae_romanae_2D_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_1_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_1_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_2_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_2_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_3_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_3_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_4_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_4_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_5_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_5_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_6_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_6_lat.txt'),\n",
       " ('/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_7_eng.txt',\n",
       "  '/Users/nickybangs/ds/metis/machine_translation/lacus_files/siculus_eclogues_7_lat.txt')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_errors = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alignments = []\n",
    "errors = []\n",
    "for pair in old_errors:\n",
    "    eng_text = open(pair[0]).read().splitlines()\n",
    "    lat_text = open(pair[1]).read().splitlines()\n",
    "    if len(eng_text) != len(lat_text):\n",
    "        errors.append(pair)    \n",
    "        continue\n",
    "\n",
    "\n",
    "    for c in range(len(eng_text)):\n",
    "        es = ' '.join(eng_text[c].replace('\\n',' ').split())\n",
    "        ls = lat_text[c].replace('\\n', ' ')\n",
    "\n",
    "        ls = re.sub(r'\\. \\. \\.','',ls)\n",
    "        es = re.sub(r'\\. \\. \\.','',es)\n",
    "        ls = re.sub('[:;,]','',ls)\n",
    "        es = re.sub('[:;,]','',es)\n",
    "\n",
    "        epar = sent_tokenize(es)\n",
    "        lpar = s_tokenizer.tokenize_sentences(ls)\n",
    "        alignments.append(get_sec_alignments(lpar,epar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for a in alignments:\n",
    "        count += len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Carthaginienses cum animadvertissent Alexandri ita magnas opes ut Africae quoque immineret unum ex civibus virum acrem nomine Hamilcarem Rhodinum iusserunt simulato exsilio ire ad regem omnique studio in amicitiam eius pervenire.',\n",
       "  'When the Carthaginians saw that the power of Alexander was so great that it menaced even Africa they ordered one of their citizens a resolute man named Hamilcar Rhodinus to go to the king pretending to be an exile and to make every effort to gain his friendship.'),\n",
       " ('Qua is potitus consilia eius nota civibus suis faciebat.',\n",
       "  'When Rhodinus had succeeded in this he disclosed to his fellow citizens the king s plans.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2809 for first set alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('../aligned_sentences/lacus_fixed_error_sentences.json','w') \n",
    "j = json.dumps(alignments)\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(dirname):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
